{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG2MLbQvZy9h"
   },
   "source": [
    "# **The Problem: Automatic Apparent Age Estimation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwmfFS5oPbhn"
   },
   "source": [
    "# Auxiliary and introductory material\n",
    "\n",
    "Getting Started with TensorFlow in Google Colaboratory\n",
    "Intro to Google Colab:\n",
    "https://www.youtube.com/watch?v=inN8seMm7UI\n",
    "\n",
    "Installing Tensorflow (CPU or GPU):\n",
    "https://www.youtube.com/watch?v=PitcORQSjNM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtnpcwuJon27"
   },
   "source": [
    "# Pre-requisites:\n",
    "Installing tensorflow-gpu (GPU) and OpenCv.\n",
    "Check GPU usage instructions [here](https://research.google.com/colaboratory/faq.html#gpu-availability)\n",
    "\n",
    "# IMPORTANT:\n",
    "**1:** THE CODE WAS TESTED ON TENSORFLOW VERSION 2.4.0\n",
    "\n",
    "**2:** Sometimes the code downloads data/models from our server. It may happen that you get a \"file not found\" error due to some instability of the server. In this case, please keep trying! If the error persist, please contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnRxTFRGorml",
    "outputId": "576357d6-892e-4737-8303-ec3ec425aa97"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==2.4.0\n",
    "# !pip install opencv-python\n",
    "# !pip install h5py\n",
    "\n",
    "# to enable Colab-GPU version:\n",
    "# 1) Runtime -> reset runtime\n",
    "# 2) Runtime -> Change runtime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMULLE13o-Qo"
   },
   "source": [
    "# Downloading and decompressing the Appa-Real Age Dataset [(source)](http://chalearnlap.cvc.uab.es/challenge/13/track/13/description/)\n",
    "\n",
    "- As default, RGB images (cropped faces) are in the range of [0, 255], and labels are in the range of ~0.9 to ~90 (years old).\n",
    "- The data is divided in train, validation and test set. \n",
    "- Matadata is also provided\n",
    "  - gender: male / female \n",
    "  - ethnicity: asian / afroamerican / caucasian\n",
    "  - facial expression: neutral / slightlyhappy / happy / other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8DZFR8GpIfL",
    "outputId": "fb0be239-f759-4c8b-b75e-6ee9be2ad040"
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "from zipfile import ZipFile\n",
    "!wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/app_data.zip\n",
    "\n",
    "# decompressing the data\n",
    "\n",
    "with ZipFile('app_data.zip', 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Data decompressed successfully')\n",
    "\n",
    "# removing the .zip file after extraction to clean space\n",
    "!rm app_data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a18X0kFGXUnt"
   },
   "source": [
    "# Loading the train/validation data, and re-scaling the labels to [0..1]\n",
    "- X_[train,valid,test] = Face images\n",
    "- Y_[train,valid,test] = Ground truth \n",
    "- M_[train,valid,test] = Metadata (gender, ethnicicy, facial expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GRhB1gzXlYc",
    "outputId": "c4aff0c1-eeaa-4331-90c6-4a87121e61cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loading the train data\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "M_train = np.load('./data/meta_data_train.npy')\n",
    "\n",
    "# loading the validation data\n",
    "X_valid = np.load('./data/data_valid.npy')\n",
    "Y_valid = np.load('./data/labels_valid.npy')\n",
    "M_valid = np.load('./data/meta_data_valid.npy')\n",
    "\n",
    "# loading the test data\n",
    "X_test = np.load('./data/data_test.npy')\n",
    "Y_test = np.load('./data/labels_test.npy')\n",
    "M_test = np.load('./data/meta_data_test.npy')\n",
    "\n",
    "# train labels are real numbers, ranging from ~0.9 to ~89 (years old);\n",
    "# we will re-scale the labels to [0,1] by using a normalization factor of 100,\n",
    "# assuming there is no sample with age > 100.\n",
    "Y_train = Y_train/100\n",
    "Y_valid = Y_valid/100\n",
    "# Y_test = Y_test/100 # -> we don't normalize the test labels as we will evaluate\n",
    "# them using the raw data, i.e., the apparent age values\n",
    "\n",
    "print('Train data size and shape', X_train.shape)\n",
    "print('Train labels size and shape', Y_train.shape)\n",
    "print('Train metadata size and shape', M_train.shape)\n",
    "print('----')\n",
    "print('Valid data size and shape', X_valid.shape)\n",
    "print('Valid labels size and shape', Y_valid.shape)\n",
    "print('Valid metadata size and shape', M_valid.shape)\n",
    "print('----')\n",
    "print('Test data size and shape', X_test.shape)\n",
    "print('Test labels size and shape', Y_test.shape)\n",
    "print('Test metadata size and shape', M_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVnh9fvUvoJ"
   },
   "source": [
    "# Visualizing some training samples\n",
    "Next, we multiply the normalized age labels by 100 to show the original age values on top of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "vxnI7M8s-JXf",
    "outputId": "fa3ce997-b041-44bd-bccd-612e9afbc15c"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = random.randint(0, len(X_train))\n",
    "    ax.imshow(cv2.cvtColor(X_train[idx, :, :, :], cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(Y_train[idx]*100)\n",
    "    ax.set(xlabel=[M_train[idx][0], M_train[idx][1], M_train[idx][2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDzps0ccD2pR"
   },
   "source": [
    "# Visualizing the age distribution of Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "jMCqsJtBD8KG",
    "outputId": "7e5c2cc9-cf59-4f6b-c7b6-324bd36a6c6b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Age distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# labels are multiplied by 100 to show the original values\n",
    "ax1.hist(Y_train*100, bins=50)\n",
    "ax1.set_title('Y_train labels')\n",
    "ax1.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax1.set_xlim([0, 100])\n",
    "\n",
    "ax2.hist(Y_valid*100, bins=50)\n",
    "ax2.set_title('Y_valid labels')\n",
    "ax2.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax2.set_xlim([0, 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uShj-2u-9HCD"
   },
   "source": [
    "# Visualizing the distributions of metadata (Train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "1SCU4nm5-qY2",
    "outputId": "b662e978-6da6-41f8-90e4-6a92ad7b4b02"
   },
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_train:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNx5EZps5WN2"
   },
   "source": [
    "# Visualizing the age distribution per Ethnicity\n",
    "- First, we define a function to visualize the age distribution per ethnicity. Then, we visualize the distributions of train / validation / test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHNdQeyT8hSa"
   },
   "outputs": [],
   "source": [
    "def compute_hist_per_ethnicity(y_data, metadata, set):\n",
    "\n",
    "    vec_as = []\n",
    "    vec_af = []\n",
    "    vec_ca = []\n",
    "    for i in range(0, len(y_data)):\n",
    "        if(metadata[i][1] == 'asian'):\n",
    "            vec_as.append(y_data[i])\n",
    "        if(metadata[i][1] == 'afroamerican'):\n",
    "            vec_af.append(y_data[i])\n",
    "        if(metadata[i][1] == 'caucasian'):\n",
    "            vec_ca.append(y_data[i])\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    fig.suptitle(['Age distribution per Ethnicity ', set],\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax1.hist(vec_as, bins=50)\n",
    "    ax1.set_xlim([0, 100])\n",
    "    ax1.set(xlabel='Asian', ylabel='Num. of samples')\n",
    "\n",
    "    ax2.hist(vec_af, bins=50)\n",
    "    ax2.set_xlim([0, 100])\n",
    "    ax2.set(xlabel='Afroamerican', ylabel='Num. of samples')\n",
    "\n",
    "    ax3.hist(vec_ca, bins=50)\n",
    "    ax3.set_xlim([0, 100])\n",
    "    ax3.set(xlabel='Caucasian', ylabel='Num. of samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "5nqiYZMF9m4S",
    "outputId": "e03dce7f-ccea-4457-dc8f-44d8f1e7df15"
   },
   "outputs": [],
   "source": [
    "# train set\n",
    "compute_hist_per_ethnicity(Y_train*100, M_train, 'Train set')\n",
    "\n",
    "# validation set\n",
    "compute_hist_per_ethnicity(Y_valid*100, M_valid, 'Validation set')\n",
    "\n",
    "# test set\n",
    "# note, we do not multiply 'Y_test' by 100 because it was not normalized\n",
    "# to be in the range of [0,1] as the train and validation sets.\n",
    "compute_hist_per_ethnicity(Y_test, M_test, 'Test set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOc3kPMYzTCI"
   },
   "source": [
    "X_train# Preprocessing the data (face images)\n",
    "- Later, we will define our model based on ResNet50 (our backbone). Originally,\n",
    "ResNet50 uses a regularization that changes the range of the input images. Thus,\n",
    "to be aligned with the ResNet50 input, we preprocess our input images using the respective 'preprocess_input' function. Later, if you decide to use another model as backbone (rather than ResNet), you may skip the following preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42bGw2eCzZ2r"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "# train\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    x = X_train[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_train[i, ] = preprocess_input(x)\n",
    "\n",
    "# validation\n",
    "for i in range(0, X_valid.shape[0]):\n",
    "    x = X_valid[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_valid[i, ] = preprocess_input(x)\n",
    "\n",
    "# test\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    x = X_test[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_test[i, ] = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlPY4wbeUr_7"
   },
   "source": [
    "---\n",
    "# Age Bias ($B_a$) \n",
    "\n",
    "- Evaluates (on the TEST set) how accurate the model is with respect to different age ranges.\n",
    "  - group 1: age < 20\n",
    "  - group 2: 20 <= age < 40\n",
    "  - group 3: 40 <= age < 60\n",
    "  - group 4: 60 <= age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umMOd4OpWP5y"
   },
   "outputs": [],
   "source": [
    "def age_bias(predictions, gt):\n",
    "    error_g1 = []\n",
    "    error_g2 = []\n",
    "    error_g3 = []\n",
    "    error_g4 = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(gt[i] < 20):\n",
    "            error_g1.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 20 and gt[i] < 40):\n",
    "            error_g2.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 40 and gt[i] < 60):\n",
    "            error_g3.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 60):\n",
    "            error_g4.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Age analysis:')\n",
    "    print('Size group 1 = %d, MAE = %f' % (len(error_g1), np.mean(error_g1)))\n",
    "    print('Size group 2 = %d, MAE = %f' % (len(error_g2), np.mean(error_g2)))\n",
    "    print('Size group 3 = %d, MAE = %f' % (len(error_g3), np.mean(error_g3)))\n",
    "    print('Size group 4 = %d, MAE = %f' % (len(error_g4), np.mean(error_g4)))\n",
    "\n",
    "    age_bias = (abs(np.mean(error_g1)-np.mean(error_g2)) +\n",
    "                abs(np.mean(error_g1)-np.mean(error_g3)) +\n",
    "                abs(np.mean(error_g1)-np.mean(error_g4)) +\n",
    "                abs(np.mean(error_g2)-np.mean(error_g3)) +\n",
    "                abs(np.mean(error_g2)-np.mean(error_g4)) +\n",
    "                abs(np.mean(error_g3)-np.mean(error_g4)))/6\n",
    "\n",
    "    print('---------')\n",
    "    print('Age bias (Ba) = ', age_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALWjE5J8edq7"
   },
   "source": [
    "# Gender Bias ($B_g$) \n",
    "- Evaluates (on the test set) how accurate the model is with respect to different gender.\n",
    "  - group 1: male\n",
    "  - group 2: female\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ua8JzFfFZK"
   },
   "outputs": [],
   "source": [
    "def gender_bias(predictions, gt, metadata):\n",
    "    error_m = []\n",
    "    error_f = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][0] == 'female'):\n",
    "            error_f.append(abs(predictions[i]-gt[i]))\n",
    "        else:\n",
    "            error_m.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Gender analysis:')\n",
    "    print('Size group female = %d, MAE = %f' %\n",
    "          (len(error_f), np.mean(error_f)))\n",
    "    print('Size group male = %d, MAE = %f' % (len(error_m), np.mean(error_m)))\n",
    "\n",
    "    gender_bias = abs(np.mean(error_f)-np.mean(error_m))\n",
    "\n",
    "    print('---------')\n",
    "    print('Gender bias (Bg) = ', gender_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8rIPUIrjkDG"
   },
   "source": [
    "# Ethnicity Bias ($B_e$)\n",
    "- Evaluates (on the test set) how accurate the model is with respect to different ethnicity categories.\n",
    "  - group 1: asian\n",
    "  - group 2: afroamerican\n",
    "  - group 3: caucasian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BnG-wK2j8nA"
   },
   "outputs": [],
   "source": [
    "def ethnicity_bias(predictions, gt, metadata):\n",
    "    error_as = []\n",
    "    error_af = []\n",
    "    error_ca = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][1] == 'asian'):\n",
    "            error_as.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][1] == 'afroamerican'):\n",
    "            error_af.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][1] == 'caucasian'):\n",
    "            error_ca.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Ethnicity Analysis:')\n",
    "    print('Size group asian = %d, MAE = %f' %\n",
    "          (len(error_as), np.mean(error_as)))\n",
    "    print('Size group afroamerican = %d, MAE = %f' %\n",
    "          (len(error_af), np.mean(error_af)))\n",
    "    print('Size group caucasian = %d, MAE = %f' %\n",
    "          (len(error_ca), np.mean(error_ca)))\n",
    "\n",
    "    ethnicity_bias = (abs(np.mean(error_as)-np.mean(error_af)) +\n",
    "                      abs(np.mean(error_as)-np.mean(error_ca)) +\n",
    "                      abs(np.mean(error_af)-np.mean(error_ca)))/3\n",
    "\n",
    "    print('---------')\n",
    "    print('Ethnicity bias (Be) = ', ethnicity_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohPkAMvxm9Ed"
   },
   "source": [
    "# Face expression bias ($B_f$)\n",
    "- Evaluates (on the test set) how accurate the model is with respect to different face expression categories.\n",
    "  - group 1: neutral\n",
    "  - group 2: slightlyhappy\n",
    "  - group 3: happy\n",
    "  - group 4: other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dJOGVMGm_Fe"
   },
   "outputs": [],
   "source": [
    "def face_expression_bias(predictions, gt, metadata):\n",
    "    error_h = []\n",
    "    error_s = []\n",
    "    error_n = []\n",
    "    error_o = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][2] == 'happy'):\n",
    "            error_h.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'slightlyhappy'):\n",
    "            error_s.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'neutral'):\n",
    "            error_n.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'other'):\n",
    "            error_o.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Face experession Analysis:')\n",
    "    print('Size group happy = %d, MAE = %f' % (len(error_h), np.mean(error_h)))\n",
    "    print('Size group slightlyhappy = %d, MAE = %f' %\n",
    "          (len(error_s), np.mean(error_s)))\n",
    "    print('Size group neutral = %d, MAE = %f' %\n",
    "          (len(error_n), np.mean(error_n)))\n",
    "    print('Size group other = %d, MAE = %f' % (len(error_o), np.mean(error_o)))\n",
    "\n",
    "    face_bias = (abs(np.mean(error_h)-np.mean(error_s)) +\n",
    "                 abs(np.mean(error_h)-np.mean(error_n)) +\n",
    "                 abs(np.mean(error_h)-np.mean(error_o)) +\n",
    "                 abs(np.mean(error_s)-np.mean(error_n)) +\n",
    "                 abs(np.mean(error_s)-np.mean(error_o)) +\n",
    "                 abs(np.mean(error_n)-np.mean(error_o)))/6\n",
    "\n",
    "    print('---------')\n",
    "    print('Face Expression bias (Bf) = ', face_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lqbRPFK62iL"
   },
   "source": [
    "---\n",
    "---\n",
    "# Strategies to improve Accuracy (i.e., to reduce the Error): \n",
    "- We will comment about two different strategies: 1) data augmentation and 2) custom loss with weighted samples.\n",
    "\n",
    "# 1) Data augmentation\n",
    "- Next, we will augment the train set of people having age >= 60, as this group got the highest MAE compared to the other groups (regarding the age attribute only). \n",
    "- Different data augmentation strategies can be used. In this example, we  consider horizontal flip, changing the brighness, gaussian blur and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "iM8PQrOcCB7n",
    "outputId": "66521924-333c-4379-eabc-709a7b690db1"
   },
   "outputs": [],
   "source": [
    "# loading the train data again (original face images, before preprocessing):\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "\n",
    "# Randomly selecting a random image from train set\n",
    "x = X_train[random.randint(0, len(X_train))]\n",
    "\n",
    "# flip horizontaly\n",
    "x_flipped = cv2.flip(x, 1)\n",
    "\n",
    "# change brightness\n",
    "x_brigth = cv2.cvtColor(x, cv2.COLOR_RGB2HSV)\n",
    "x_brigth[:, :, 2] = x_brigth[:, :, 2]*.5+np.random.uniform()\n",
    "x_brigth = cv2.cvtColor(x_brigth, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "# gaussian blur (here you can also play with the function parameters)\n",
    "x_blur = cv2.GaussianBlur(x, (5, 5), 1.0)\n",
    "\n",
    "# translation (randomly translation from -25 to +25 in x and y)\n",
    "rows, cols, c = x.shape\n",
    "M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "               [0, 1, random.randint(-25, 25)]])\n",
    "x_translate = cv2.warpAffine(x, M, (cols, rows))\n",
    "\n",
    "# Visualizing the augmented data\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 15))\n",
    "ax1.imshow(cv2.cvtColor(x, cv2.COLOR_BGR2RGB))  # original image\n",
    "ax2.imshow(cv2.cvtColor(x_flipped, cv2.COLOR_BGR2RGB))  # flip horizontaly\n",
    "ax3.imshow(cv2.cvtColor(x_brigth, cv2.COLOR_BGR2RGB))  # change brightness\n",
    "ax4.imshow(cv2.cvtColor(x_blur, cv2.COLOR_BGR2RGB))  # gaussian blur\n",
    "ax5.imshow(cv2.cvtColor(x_translate, cv2.COLOR_BGR2RGB))  # translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY2uBi2L-ro-"
   },
   "source": [
    "# Applying the data augmentation described above\n",
    "- Note, for each augmented image, we replicate its original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMcDuU4xFvNv",
    "outputId": "c1ce65e2-4cfe-49e5-91eb-53643578aaf2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# aux variables\n",
    "X_train_augmented = []\n",
    "Y_train_augmented = []\n",
    "\n",
    "# loading the train data and labels\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "Y_train = Y_train/100\n",
    "print('Train set before augmentation = ', np.array(X_train).shape)\n",
    "\n",
    "# augmenting the data\n",
    "for i in range(0,len(X_train)):\n",
    "  # check if image is in the group 'age >= 60'\n",
    "\n",
    "  if Y_train[i]*100>=60: # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "    # flip\n",
    "    X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "    Y_train_augmented.append(Y_train[i]) \n",
    "\n",
    "    # changing brightness\n",
    "    x_aux = cv2.cvtColor(X_train[i],cv2.COLOR_RGB2HSV)\n",
    "    x_aux[:,:,2] = x_aux[:,:,2]*.5+np.random.uniform()\n",
    "    X_train_augmented.append(cv2.cvtColor(x_aux,cv2.COLOR_HSV2RGB))\n",
    "    Y_train_augmented.append(Y_train[i]) \n",
    "\n",
    "    # blur\n",
    "    X_train_augmented.append(cv2.GaussianBlur(X_train[i],(5,5),1.0))\n",
    "    Y_train_augmented.append(Y_train[i])\n",
    "   \n",
    "    # translation\n",
    "    rows, cols ,c= X_train[i].shape\n",
    "    M = np.float32([[1, 0, random.randint(-25, 25)], [0, 1, random.randint(-25, 25)]])\n",
    "    X_train_augmented.append(cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "    Y_train_augmented.append(Y_train[i])\n",
    "\n",
    "# adding the augmented images to the train set\n",
    "X_train = np.concatenate((X_train, X_train_augmented))\n",
    "Y_train = np.concatenate((Y_train, Y_train_augmented))\n",
    "print('Train set after augmentation = ', np.array(X_train).shape)\n",
    "\n",
    "\n",
    "# post-processing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "# aux variables\n",
    "X_train_augmented = []\n",
    "Y_train_augmented = []\n",
    "M_train_augmented = []\n",
    "\n",
    "# loading the train data and labels\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "M_train = np.load('./data/meta_data_train.npy')\n",
    "\n",
    "Y_train = Y_train/100\n",
    "print('Train set before augmentation = ', np.array(X_train).shape)\n",
    "\n",
    "# augmenting the data\n",
    "for i in range(0, len(X_train)):\n",
    "  # check if image is in the group 'age >= 60'\n",
    "    # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "    if Y_train[i]*100 >= 60 and M_train[i][1] != 'caucasian':\n",
    "        for n in range(3):\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2]*.5+np.random.uniform()\n",
    "            X_train_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (5, 5), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (3, 3), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "    # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "    if (Y_train[i]*100 >= 60 and (M_train[i][2] == 'happy' or M_train[i][2] == 'other')) or M_train[i][1] == 'afroamerican':\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2]*.5+np.random.uniform()\n",
    "            X_train_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (5, 5), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (3, 3), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "    # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "    if ((60 > Y_train[i]*100 >= 40 or 18 > Y_train[i]*100) and (M_train[i][2] == 'happy' or M_train[i][2] == 'other')) or M_train[i][1] == 'asian':\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2]*.5+np.random.uniform()\n",
    "            X_train_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (5, 5), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "    if ((100 > Y_train[i]*100 >= 40 or 18 > Y_train[i]*100) and (M_train[i][2] == 'neutral' or M_train[i][2] == 'other')) or M_train[i][1] == 'asian':\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2]*.5+np.random.uniform()\n",
    "            X_train_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (5, 5), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "    \n",
    "    if ((100 > Y_train[i]*100 >= 40 or 18 > Y_train[i]*100) and (M_train[i][2] == 'happy' or M_train[i][2] == 'other')) or M_train[i][1] == 'afroamerican':\n",
    "        for n in range(3):\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # flip\n",
    "            X_train_augmented.append(cv2.flip(X_train[i], 0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_train[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2]*.5+np.random.uniform()\n",
    "            X_train_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # blur\n",
    "            X_train_augmented.append(cv2.GaussianBlur(X_train[i], (5, 5), 1.0))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_train[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_train_augmented.append(\n",
    "                cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
    "            Y_train_augmented.append(Y_train[i])\n",
    "            M_train_augmented.append(M_train[i])\n",
    "\n",
    "# adding the augmented images to the train set\n",
    "X_train = np.concatenate((X_train, X_train_augmented))\n",
    "Y_train = np.concatenate((Y_train, Y_train_augmented))\n",
    "M_train = np.concatenate((M_train, M_train_augmented))\n",
    "\n",
    "print('Train set after augmentation = ', np.array(X_train).shape)\n",
    "\n",
    "\n",
    "# post-processing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    x = X_train[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_train[i, ] = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_train:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution TRAIN', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux variables\n",
    "X_valid_augmented = []\n",
    "Y_valid_augmented = []\n",
    "M_valid_augmented = []\n",
    "\n",
    "# loading the valid data and labels\n",
    "X_valid = np.load('./data/data_valid.npy')\n",
    "Y_valid = np.load('./data/labels_valid.npy')\n",
    "M_valid = np.load('./data/meta_data_valid.npy')\n",
    "\n",
    "Y_valid = Y_valid / 100\n",
    "print('Valid set before augmentation = ', np.array(X_valid).shape)\n",
    "\n",
    "# augmenting the data\n",
    "for i in range(0, len(X_valid)):\n",
    "    # check if image is in the group 'age >= 60'\n",
    "    # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "    if Y_valid[i] * 100 >= 60 and M_valid[i][1] != 'caucasian':\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 1))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_valid[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2] * .5 + np.random.uniform()\n",
    "            X_valid_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (5, 5), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (3, 3), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "    if (Y_valid[i] * 100 >= 60 and (M_valid[i][2] == 'happy' or M_valid[i][2] == 'other')) or M_valid[i][\n",
    "            1] == 'afroamerican':  # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 1))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_valid[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2] * .5 + np.random.uniform()\n",
    "            X_valid_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (5, 5), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (3, 3), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[0, 1, random.randint(-25, 25)],\n",
    "                           [1, 0, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "    if ((100 > Y_valid[i] * 100 >= 40 or 18 > Y_valid[i] * 100) and (M_valid[i][2] == 'happy' or M_valid[i][2] == 'other')) or M_valid[i][1] == 'asian':  # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 1))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_valid[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2] * .5 + np.random.uniform()\n",
    "            X_valid_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (5, 5), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "            \n",
    "    if ((100 > Y_valid[i] * 100 >= 40 or 18 > Y_valid[i] * 100) and (M_valid[i][2] == 'neutral' or M_valid[i][2] == 'slightyhappy')) or M_valid[i][1] == 'asian':  # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
    "        for n in range(2):\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 1))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # flip\n",
    "            X_valid_augmented.append(cv2.flip(X_valid[i], 0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # changing brightness\n",
    "            x_aux = cv2.cvtColor(X_valid[i], cv2.COLOR_RGB2HSV)\n",
    "            x_aux[:, :, 2] = x_aux[:, :, 2] * .5 + np.random.uniform()\n",
    "            X_valid_augmented.append(cv2.cvtColor(x_aux, cv2.COLOR_HSV2RGB))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # blur\n",
    "            X_valid_augmented.append(cv2.GaussianBlur(X_valid[i], (5, 5), 1.0))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "            # translation\n",
    "            rows, cols, c = X_valid[i].shape\n",
    "            M = np.float32([[1, 0, random.randint(-25, 25)],\n",
    "                           [0, 1, random.randint(-25, 25)]])\n",
    "            X_valid_augmented.append(\n",
    "                cv2.warpAffine(X_valid[i], M, (cols, rows)))\n",
    "            Y_valid_augmented.append(Y_valid[i])\n",
    "            M_valid_augmented.append(M_valid[i])\n",
    "\n",
    "    # adding the augmented images to the valid set\n",
    "X_valid = np.concatenate((X_valid, X_valid_augmented))\n",
    "Y_valid = np.concatenate((Y_valid, Y_valid_augmented))\n",
    "M_valid = np.concatenate((M_valid, M_valid_augmented))\n",
    "\n",
    "print('Valid set after augmentation = ', np.array(X_valid).shape)\n",
    "\n",
    "# post-processing the valid data with respect to ResNet-50 Inputs.\n",
    "for i in range(0, X_valid.shape[0]):\n",
    "    x = X_valid[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_valid[i, ] = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Age distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# labels are multiplied by 100 to show the original values\n",
    "ax1.hist(Y_train*100, bins=50)\n",
    "ax1.set_title('Y_train labels')\n",
    "ax1.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax1.set_xlim([0, 100])\n",
    "\n",
    "# labels are multiplied by 100 to show the original values\n",
    "ax2.hist(Y_valid*100, bins=50)\n",
    "ax2.set_title('Y_valid labels')\n",
    "ax2.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax2.set_xlim([0, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_train:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution TRAIN', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_valid:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution VALID', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_test:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution TEST', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "compute_hist_per_ethnicity(Y_train*100, M_train, 'Train set')\n",
    "\n",
    "# validation set\n",
    "compute_hist_per_ethnicity(Y_valid*100, M_valid, 'Validation set')\n",
    "\n",
    "# test set\n",
    "# note, we do not multiply 'Y_test' by 100 because it was not normalized\n",
    "# to be in the range of [0,1] as the train and validation sets.\n",
    "compute_hist_per_ethnicity(Y_test, M_test, 'Test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"standard deaviation train\", np.std(Y_train*100))\n",
    "print(\"mean train\", np.mean(Y_train*100))\n",
    "print(\"standard deaviation valid\", np.std(Y_valid*100))\n",
    "print(\"mean valid\", np.mean(Y_valid*100))\n",
    "print(\"standard deaviation test\", np.std(Y_test))\n",
    "print(\"mean test\", np.mean(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING FIRST STAGE, LAST LAYERS (FINE TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# loading the pretrained model\n",
    "# model = tf.keras.models.load_model('./model/weights.h5')\n",
    "base_model = tf.keras.applications.EfficientNetV2S(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
    "# print the model summary\n",
    "#print(base_model.summary())\n",
    "base_model.trainable = False\n",
    "\n",
    "# Using the FC layer before the 'classifier_low_dim' layer as feature vector\n",
    "conv2d = tf.keras.layers.Conv2D(7, 7)(base_model.output, training=False)\n",
    "bn = tf.keras.layers.BatchNormalization()(conv2d)\n",
    "gap = tf.keras.layers.GlobalAveragePooling2D()(bn)\n",
    "do = Dropout(0.2)(gap)\n",
    "flatten = Flatten()(do)\n",
    "fc1 = Dense(512, activation='relu')(flatten)\n",
    "do2 = Dropout(0.2)(fc1)\n",
    "fc2 = Dense(256, activation='relu')(do2)\n",
    "output = Dense(1, activation='relu', name='predict')(fc2)\n",
    "\n",
    "# building and pringing the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "# print(model.summary())\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "# load a model and train history (defined and trained\n",
    "# as below, trained for 38 epochs)\n",
    "# --------------------------\n",
    "LOAD_BEST_MODEL_ST1 = False  # (training only the last FC layers)\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    # downloading the trained model\n",
    "    !wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st1.zip\n",
    "    # decompressing the data\n",
    "    with ZipFile('best_model_st1.zip', 'r') as zip:\n",
    "        zip.extractall()\n",
    "        print('Model decompressed successfully')\n",
    "    # removing the .zip file after extraction  to clean space\n",
    "    !rm best_model_st1.zip\n",
    "\n",
    "else:\n",
    "    # defining the early stop criteria\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    # saving the best model based on val_loss\n",
    "    mc = ModelCheckpoint('./checkpoint/best_model.h5',\n",
    "                         monitor='val_loss', mode='min', save_best_only=True)\n",
    "    mc_2 = ModelCheckpoint('./checkpoint/best_model.h5',\n",
    "                           monitor='val_loss', mode='min', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10,  min_lr=1e-6)\n",
    "\n",
    "    # defining the optimizer\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4, amsgrad=False),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
    "\n",
    "    # training the model\n",
    "    history = model.fit(X_train, Y_train, validation_data=(\n",
    "        X_valid, Y_valid), batch_size=16, epochs=50, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n",
    "\n",
    "    # saving training history (for future visualization)\n",
    "    with open('./history/train_history.pkl', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# here, it loads the history of the model we have already trained, or loads the\n",
    "# history of the model you defined and trained\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    train_hist = pickle.load(open(\"./history/train_history.pkl\", \"rb\"))\n",
    "else:\n",
    "    train_hist = pickle.load(open(\"./history/train_history.pkl\", \"rb\"))\n",
    "\n",
    "# we plot both, the LOSS and MAE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Training history (stage 1)', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.plot(train_hist['loss'])\n",
    "ax1.plot(train_hist['val_loss'])\n",
    "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
    "ax1.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "ax2.plot(train_hist['mae'])\n",
    "ax2.plot(train_hist['val_mae'])\n",
    "ax2.set(xlabel='epoch', ylabel='MAE')\n",
    "ax2.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_ST1 = True\n",
    "# --------------------------\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# loading the saved model\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    saved_model = load_model('best_model.h5')\n",
    "else:\n",
    "    saved_model = load_model('./checkpoint/best_model.h5')\n",
    "\n",
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # predict on the test data\n",
    "    predictions_st1 = saved_model.predict(X_test, batch_size=32, verbose=1)\n",
    "    \n",
    "\n",
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_st1_f = predictions_st1*100\n",
    "\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(abs(np.subtract(predictions_st1_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n",
    "\n",
    "\n",
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # printing some predictions\n",
    "    for i in range(0, 10):\n",
    "        print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "              (predictions_st1_f[i], Y_test[i]))\n",
    "        \n",
    "# computing the age bias (model_stage_2)\n",
    "age_bias(predictions_st1_f, Y_test)\n",
    "\n",
    "# computing the gender bias (model_stage_2)\n",
    "gender_bias(predictions_st1_f, Y_test, M_test)\n",
    "\n",
    "# computing the ethnicity bias (model_stage_2)\n",
    "ethnicity_bias(predictions_st1_f, Y_test, M_test)\n",
    "\n",
    "# computing the face bias (model_stage_2)\n",
    "face_expression_bias(predictions_st1_f, Y_test, M_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# saving the predictions as a csv file\n",
    "with open('predictions.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(predictions_st1_f)\n",
    "csvFile.close()\n",
    "\n",
    "# compressing the csv file (to be submitted to codalab as prediction)\n",
    "! zip predictions.zip predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjkJNXGilNfo"
   },
   "source": [
    "---\n",
    "---\n",
    "# Strategies to improve Accuracy (i.e., to reduce the Error):\n",
    "# 2) Custom Loss: sample weights to deal with inbalanced categories\n",
    "- Next, we will created a \"customized loss\", which gives more weight to people having less samples in train data. For this, **we will consider the age range only**. This way, we believe the model will be able to generalize a little bit better to those particular groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGjU8Xetxl3W"
   },
   "source": [
    "# Load the Train data again (to remove the augmented data) and generate the weigths \n",
    "- First, we will generate a weight for each age group (for g =1 to 4);\n",
    "- The formula used to calculate the weight for each group $j$ is:\n",
    "\n",
    "  $w_j=n_{samples} / (n_{classes} * n_{samples,j}),$\n",
    "\n",
    "  Where\n",
    "\n",
    "    - $w_j$ is the weight for each group $j$,\n",
    "    - $n_{samples}$ is the number of samples in the train set,\n",
    "    - $n_{classes}$ is the number of classes (4 in our case, as we divided the ages in 4 groups),\n",
    "    - $n_{samples,j}$ is the number of samples of class (group) $j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svMMpvU9xcJR",
    "outputId": "35314142-5999-4079-a06c-f197cdfb2bf9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rom tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = 0\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "      g1 +=1\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "      g2 +=1\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "      g3 +=1\n",
    "    if(Y_train[i]*100>=60):\n",
    "      g4 +=1\n",
    "print('group(s) size = ', [g1, g2, g3, g4])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "      sample_weights.append(w[0])\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "      sample_weights.append(w[1])\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "      sample_weights.append(w[2])\n",
    "    if(Y_train[i]*100>=60):\n",
    "      sample_weights.append(w[3])\n",
    "sample_weights = np.array(sample_weights)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "# X_train = np.load('./data/data_train.npy')\n",
    "# Y_train = np.load('./data/labels_train.npy')\n",
    "# M_train = np.load('./data/meta_data_train.npy')\n",
    "# Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "\"\"\"\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\"\"\"\n",
    "\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = g5 = g6 = g7 = g8 = g9 = g10 = g11 = g12 = g13 = g14 = g15 = g16 = g17 = g18 = g19 = g20 = g21 = g22 = g23 = g24 = 1\n",
    "g25 = g26 = g27 = g28 = g29 = g30 = g31 = g32 = g33 = g34 = g35 = g36 = g37 = g38 = g39 = g40 = 1\n",
    "g41 = g42 = g43 = g44 = g45 = g46 = g47 = g48 = 1\n",
    "for i in range(0, Y_train.shape[0]):\n",
    "    if(Y_train[i]*100 < 20):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g1 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g2 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g3 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g4 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g5 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g6 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g7 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g8 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g9 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g10 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g11 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g12 += 1\n",
    "    if(Y_train[i]*100 >= 20 and Y_train[i]*100 < 40):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g13 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g14 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g15 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g16 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g17 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g18 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g19 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g20 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g21 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g22 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g23 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g24 += 1\n",
    "    if(Y_train[i]*100 >= 40 and Y_train[i]*100 < 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g25 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g26 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g27 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g28 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g29 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g30 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g31 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g32 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g33 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g34 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g35 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g36 += 1\n",
    "    if(Y_train[i]*100 >= 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g37 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g38 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g39 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g40 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g41 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g42 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g43 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g44 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g45 += 1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g46 += 1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g47 += 1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g48 += 1\n",
    "print('group(s) size = ', [g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, g15, g16, g17, g18, g19, g20, g21, g22, g23, g24,\n",
    "                           g25, g26, g27, g28, g29, g30, g31, g32, g33, g34, g35, g36, g37, g38, g39, g40,\n",
    "                           g41, g42, g43, g44, g45, g46, g47, g48])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, g15, g16, g17, g18, g19, g20, g21, g22, g23, g24,\n",
    "                  g25, g26, g27, g28, g29, g30, g31, g32, g33, g34, g35, g36, g37, g38, g39, g40,\n",
    "                  g41, g42, g43, g44, g45, g46, g47, g48]))/(48*np.array([g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, g15, g16, g17, g18, g19, g20, g21, g22, g23, g24,\n",
    "                                                                          g25, g26, g27, g28, g29, g30, g31, g32, g33, g34, g35, g36, g37, g38, g39, g40,\n",
    "                                                                          g41, g42, g43, g44, g45, g46, g47, g48]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0, Y_train.shape[0]):\n",
    "    if(Y_train[i]*100 < 20):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[0])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[1])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[2])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[3])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[4])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[5])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[6])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[7])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[8])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[9])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[10])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[11])\n",
    "    if(Y_train[i]*100 >= 20 and Y_train[i]*100 < 40):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[12])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[13])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[14])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[15])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[16])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[17])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[18])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[19])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[20])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[21])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[22])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[23])\n",
    "    if(Y_train[i]*100 >= 40 and Y_train[i]*100 < 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[24])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[25])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[26])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[27])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[28])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[29])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[30])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[31])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[32])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[33])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[34])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[35])\n",
    "    if(Y_train[i]*100 >= 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[36])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[37])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[38])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[39])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[40])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[41])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[42])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[43])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[44])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[45])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[46])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[47])\n",
    "sample_weights = np.array(sample_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "# X_train = np.load('./data/data_train.npy')\n",
    "# Y_train = np.load('./data/labels_train.npy')\n",
    "# M_train = np.load('./data/meta_data_train.npy')\n",
    "# Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "\"\"\"\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\"\"\"\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = g5 = g6 = g7 = g8 = g9 = g10 = 0\n",
    "for i in range(0, Y_train.shape[0]):\n",
    "    if(Y_train[i]*100 < 20):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            g1 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            g2 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            g3 += 1\n",
    "    if(Y_train[i]*100 >= 20 and Y_train[i]*100 < 40):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            g4 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            g5 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            g6 += 1\n",
    "    if(Y_train[i]*100 >= 40 and Y_train[i]*100 < 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            g7 += 1\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            g8 += 1\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            g9 += 1\n",
    "    if(Y_train[i]*100 >= 60):\n",
    "        g10 += 1\n",
    "\n",
    "print('group(s) size = ', [g1, g2, g3, g4, g5, g6, g7, g8, g9, g10])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1, g2, g3, g4, g5, g6, g7, g8, g9, g10])) / \\\n",
    "    (10*np.array([g1, g2, g3, g4, g5, g6, g7, g8, g9, g10]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0, Y_train.shape[0]):\n",
    "    if(Y_train[i]*100 < 20):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            sample_weights.append(w[0])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            sample_weights.append(w[1])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            sample_weights.append(w[2])\n",
    "    if(Y_train[i]*100 >= 20 and Y_train[i]*100 < 40):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            sample_weights.append(w[3])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            sample_weights.append(w[4])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            sample_weights.append(w[5])\n",
    "    if(Y_train[i]*100 >= 40 and Y_train[i]*100 < 60):\n",
    "        if(M_train[i][1] == 'caucasian'):\n",
    "            sample_weights.append(w[6])\n",
    "        if(M_train[i][1] == 'asian'):\n",
    "            sample_weights.append(w[7])\n",
    "        if(M_train[i][1] == 'afroamerican'):\n",
    "            sample_weights.append(w[8])\n",
    "    if(Y_train[i]*100 >= 60):\n",
    "        sample_weights.append(w[9])\n",
    "sample_weights = np.array(sample_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gTtMXGmx5jA"
   },
   "source": [
    "# I) Using the SAMPLE WEIGHTS to train our model and,\n",
    "- Next, you will see the code we used to train our model (2nd stage) from the model we obtained at the 1st stage, using the customized loss option with sample weights.\n",
    "- As default, the code will load the model already trained. \n",
    "- You can change the boolean variable 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS' to False to train your model.\n",
    "- Note, now we include other variables ('RESUME_TRAINING' and 'RESUME_FROM_EPOCH') to allow us resuming training, as well as to inform from what epoch we want to resume the trainind, detailed below.\n",
    "\n",
    "# II) illustrating how to train + save + stop training + RESUME TRAINING\n",
    "- **Imagine** you set 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = False', 'NUM_EPOCHS = 12' and 'RESUME_TRAINING = False' to train your model the first time.\n",
    "- Due to Colab limitations, your process stoped the training at the middle of epoch 10, and you saved the best model based on validation loss on epoch 9.\n",
    "- In the above example, you can resume training from epoch 9 by setting the following parameters:\n",
    "  - 'RESUME_TRAINING = True'\n",
    "  - 'RESUME_FROM_EPOCH = 9'\n",
    "\n",
    "- IMPORTANT: to resume training, you will need to monitor the epoch number where your model stopped before resuming the training, and change the defined variables properly. \n",
    "  - Note that the fit function is adapted to receive the sample weights ('sample_weight=sample_weights').\n",
    "  - Also note that the fit function changes if you are training from epoch 0 (initial_epoch=0) or resume training (initial_epoch=RESUME_FROM_EPOCH). \n",
    "  - Finally, note that when you are resuming training, you load your 'best_model_2nd_stage_weighted.h5' instead of the model trained at stage 1 ('best_model.h5').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_train)  # DATA AUGMENTATION 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjEx3B3lx8Mu",
    "outputId": "fc038ac4-299d-4680-fc7e-06a1cad294aa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "# --------------------------\n",
    "LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = False\n",
    "NUM_EPOCHS = 100\n",
    "# --------------------------\n",
    "RESUME_TRAINING = False\n",
    "RESUME_FROM_EPOCH = 9\n",
    "# --------------------------\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    # downloading the trained model\n",
    "    !wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_weighted.zip\n",
    "    # decompressing the data\n",
    "    with ZipFile('best_model_weighted.zip', 'r') as zip:\n",
    "        zip.extractall()\n",
    "        print('Model decompressed successfully')\n",
    "    # removing the .zip file after extraction  to clean space\n",
    "    !rm best_model_weighted.zip\n",
    "\n",
    "else:\n",
    "    # loading the saved model (best model learned at stage 1)\n",
    "    if(RESUME_TRAINING == False):\n",
    "        # load model from stage 1 best_model\n",
    "        saved_model = tf.keras.models.load_model('./checkpoint/best_model.h5')\n",
    "    else:\n",
    "        # resume training (stage 2)\n",
    "        saved_model = tf.keras.models.keras.load_model('./checkpoint/best_model.h5')\n",
    "\n",
    "    # setting all layers to traineble\n",
    "    counter = 0\n",
    "    for layer in saved_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "        # print(counter, layer.name, layer.trainable)\n",
    "        counter += 1\n",
    "\n",
    "    # =================================================\n",
    "    # training all layers (2nd stage), given the model saved on stage 1\n",
    "    saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4, amsgrad=False), loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
    "    # =================================================\n",
    "\n",
    "    # defining the early stop criteria\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    mc = ModelCheckpoint('./checkpoint/best_model_2nd_stage_weighted.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    mc_2 = ModelCheckpoint('./checkpoint/best_model_2nd_stage_weighted_mae.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6, min_lr=1e-6)\n",
    "\n",
    "    if(RESUME_TRAINING == False):\n",
    "        history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(\n",
    "            X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n",
    "    else:\n",
    "        history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(\n",
    "            X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n",
    "        \n",
    "    with open('./history/train_history_weights.pkl', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# here, it loads the history of the model we have already trained, or loads the\n",
    "# history of the model you defined and trained\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    train_hist = pickle.load(open(\"./history/train_history_weights.pkl\", \"rb\"))\n",
    "else:\n",
    "    train_hist = pickle.load(open(\"./history/train_history_weights.pkl\", \"rb\"))\n",
    "\n",
    "# we plot both, the LOSS and MAE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Training history (STAGE 2 WITH DATA AUGMENTATION AND WEIGHTS)', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.plot(train_hist['loss'])\n",
    "ax1.plot(train_hist['val_loss'])\n",
    "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
    "ax1.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "ax2.plot(train_hist['mae'])\n",
    "ax2.plot(train_hist['val_mae'])\n",
    "ax2.set(xlabel='epoch', ylabel='MAE')\n",
    "ax2.legend(['train', 'valid'], loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7kqieNu7zDS"
   },
   "source": [
    "# Making predictions on the Test set and Evaluating\n",
    "- Note that in this case, the model obtained MAE = 12.45038828, which is not the best score compared to the ones obtained before. However, are the evaluated biases better? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYq1tCet70bN",
    "outputId": "3111c657-af7c-4de6-9152-c34edb81075c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    saved_model_2nd_weighted = load_model('best_model_2nd_stage_weighted.h5')\n",
    "else:\n",
    "    saved_model_2nd_weighted = load_model(\n",
    "        './checkpoint/best_model_2nd_stage_weighted.h5')\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_WEIGHTED = True\n",
    "# --------------------------\n",
    "\n",
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # predict on the test data\n",
    "    predictions_2nd_weighted = saved_model_2nd_weighted.predict(\n",
    "        X_test, batch_size=32, verbose=1)\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_2nd_weighted_f = predictions_2nd_weighted*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCyMYVPgILxq",
    "outputId": "d5a3f622-8de6-42e2-9c68-243288f54588"
   },
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(\n",
    "            abs(np.subtract(predictions_2nd_weighted_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RD-J3swDIjQW",
    "outputId": "c011877a-a70c-45a9-8e05-5cfc1ba17d98"
   },
   "outputs": [],
   "source": [
    "# printing some predictions\n",
    "for i in range(0, 20):\n",
    "    print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "          (predictions_2nd_weighted_f[i], Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpJCmvGQXDIS"
   },
   "source": [
    "# Comparing the 2nd stage of training: \n",
    "case a) without augmentation and custom loss **VS.** case b) without augmentation but with custom loss.\n",
    "- Age bias:\n",
    "  - case a: 8.988896687825521\n",
    "  - case b: 3.965182622273763\n",
    "- Gender bias:\n",
    "  - case a: 0.6280031\n",
    "  - case b: 0.54932594\n",
    "- Ethnicity bias:\n",
    "  - case a: 2.447519620259603\n",
    "  - case b: 2.094111124674479\n",
    "- Face Expression bias:\n",
    "  - case a: 0.8196892738342285\n",
    "  - case b: 1.220861275990804\n",
    "\n",
    "As it can be observed, the model with custom loss and weighted samples obtained overall smaller bias scores on all evaluated attributes (except for face expression), even if the weigts were defined based on age attribute only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjrDG9AHLoPf",
    "outputId": "ca7130e8-a745-4ace-ca85-9067cae44bb3"
   },
   "outputs": [],
   "source": [
    "\n",
    "age_bias(predictions_2nd_weighted_f, Y_test)\n",
    "\n",
    "gender_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "ethnicity_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "face_expression_bias(predictions_2nd_weighted_f, Y_test, M_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# saving the predictions as a csv file\n",
    "with open('predictions.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(predictions_2nd_weighted_f)\n",
    "    csvFile.close()\n",
    "\n",
    "# compressing the csv file (to be submitted to codalab as prediction)\n",
    "! zip predictions.zip predictions.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    saved_model_2nd_weighted = load_model(\n",
    "        'best_model_2nd_stage_weighted_mae.h5')\n",
    "else:\n",
    "    saved_model_2nd_weighted = load_model(\n",
    "        './checkpoint/best_model_2nd_stage_weighted_mae.h5')\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_WEIGHTED = True\n",
    "# --------------------------\n",
    "\n",
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # predict on the test data\n",
    "    predictions_2nd_weighted = saved_model_2nd_weighted.predict(\n",
    "        X_test, batch_size=32, verbose=1)\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_2nd_weighted_f = predictions_2nd_weighted*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(\n",
    "            abs(np.subtract(predictions_2nd_weighted_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing some predictions\n",
    "for i in range(0, 20):\n",
    "    print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "          (predictions_2nd_weighted_f[i], Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bias(predictions_2nd_weighted_f, Y_test)\n",
    "\n",
    "gender_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "ethnicity_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "face_expression_bias(predictions_2nd_weighted_f, Y_test, M_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# saving the predictions as a csv file\n",
    "with open('predictions.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(predictions_2nd_weighted_f)\n",
    "    csvFile.close()\n",
    "\n",
    "# compressing the csv file (to be submitted to codalab as prediction)\n",
    "! zip predictions_2.zip predictions.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-sTrrsB4oR6"
   },
   "source": [
    "---\n",
    "---\n",
    "# Practical Exercises \n",
    "Next, we define a serie of practical exercises (Task 1 and 2, and an optional extra exercise).  **Your goal is to maximize accuracy (i.e., reduce the Mean Absolute Error) and minimize the evaluated bias scores on the different attributes**. Task 1 and 2 have some restrictions so that you can compare the results when following different strategies. Note, you can edit and improve the starting kit on each task, but you are free to start from strach and create a new solution. At the end, you will be evaluated based on a set of items (detailed in the practical classes) and **creativity**. \n",
    "\n",
    "- IMPORTANT: we will use **Codalab** to motivate the students, as they can submit their results on the platform, compete with each other and improve their solutions, but the ranking shown in the leaderboard will not be considered in the evaluation. This is to justify that more creative solutions will be priefered even if they don't provide the best results.\n",
    "- Note: you will be requested to share with the lectors (Sergio and Julio) your final **Colab file** (with a clean code and well documented) and a **Report document** where you describe your experiments and solution, compare and discuss the obtained results in a progressive and clear way. Please, check the class material associated to the practical sessions for more details. \n",
    "\n",
    "---\n",
    "- **Task 1 (with data augmentation):** For this task, you should define your model (e.g., a generic backbone with some small changes to solve the problem at hand, like including/removing some layers, etc), play with the different hyperparameters (e.g., number of epochs, learning rate, batch size, etc), regularizers (e.g., dropout layer), loos function (e.g., MSE, MAE, etc). You can also play with the training strategy (e.g., training using different stages - or not - freezing different layers during training - or not, etc). Then, **you will be requested to perform some data augmentation** to achieve your goal. Note, you could simply expand the idea of the starting kit to cover other attributes (e.g., age > 40 or for \"happy\" expression). However, we expect more creative solutions, where different approaches are employed (e.g., new transformations, covering different attributes, etc). Then, you should submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "\n",
    "- **Task 2 (custom loos, without data augmentation):** For this task, you should fix the model employed in Task 1, but you can also play with the different hyperparameters, regularizers, loos function and training strategy. Then, **you will be requested to use a custom loss (e.g., sample weights)** to achieve your goal, **without any data augmentation** method. This way, you will be able to compare the different solutions (Task 1 vs. Task 2). Then, you should submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "\n",
    "- **Extra (optional):** For this task, you can exploit your creativity as much as you can. You are free to employ any strategy, data augmentation, custom loss, etc, all together in order to achieve your goal. Then, you will be able to compare the obtained results for Task 1 vs. Task 2 vs. the extra (optional) task. Then, you can also submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "---\n",
    "\n",
    "\n",
    "**Codalab Competition link:** https://codalab.lisn.upsaclay.fr/competitions/2321?secret_key=b66c95cb-997c-4fc9-af4e-987721abfa6c \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dG2MLbQvZy9h",
    "XwmfFS5oPbhn",
    "BXQZXojAxGJc",
    "_UY-ci6Yc7VR",
    "xjkJNXGilNfo",
    "F-sTrrsB4oR6"
   ],
   "name": "2022_UB-Master_Colab_introduction_age.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
