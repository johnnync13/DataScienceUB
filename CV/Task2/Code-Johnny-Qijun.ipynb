{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG2MLbQvZy9h"
   },
   "source": [
    "# **The Problem: Automatic Apparent Age Estimation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwmfFS5oPbhn"
   },
   "source": [
    "# Auxiliary and introductory material\n",
    "\n",
    "Getting Started with TensorFlow in Google Colaboratory\n",
    "Intro to Google Colab:\n",
    "https://www.youtube.com/watch?v=inN8seMm7UI\n",
    "\n",
    "Installing Tensorflow (CPU or GPU):\n",
    "https://www.youtube.com/watch?v=PitcORQSjNM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtnpcwuJon27"
   },
   "source": [
    "# Pre-requisites:\n",
    "Installing tensorflow-gpu (GPU) and OpenCv.\n",
    "Check GPU usage instructions [here](https://research.google.com/colaboratory/faq.html#gpu-availability)\n",
    "\n",
    "# IMPORTANT:\n",
    "**1:** THE CODE WAS TESTED ON TENSORFLOW VERSION 2.4.0\n",
    "\n",
    "**2:** Sometimes the code downloads data/models from our server. It may happen that you get a \"file not found\" error due to some instability of the server. In this case, please keep trying! If the error persist, please contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnRxTFRGorml",
    "outputId": "576357d6-892e-4737-8303-ec3ec425aa97"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu==2.4.0\n",
    "#!pip install opencv-python\n",
    "#!pip install h5py\n",
    "\n",
    "# to enable Colab-GPU version:\n",
    "# 1) Runtime -> reset runtime\n",
    "# 2) Runtime -> Change runtime type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMULLE13o-Qo"
   },
   "source": [
    "# Downloading and decompressing the Appa-Real Age Dataset [(source)](http://chalearnlap.cvc.uab.es/challenge/13/track/13/description/)\n",
    "\n",
    "- As default, RGB images (cropped faces) are in the range of [0, 255], and labels are in the range of ~0.9 to ~90 (years old).\n",
    "- The data is divided in train, validation and test set. \n",
    "- Matadata is also provided\n",
    "  - gender: male / female \n",
    "  - ethnicity: asian / afroamerican / caucasian\n",
    "  - facial expression: neutral / slightlyhappy / happy / other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8DZFR8GpIfL",
    "outputId": "fb0be239-f759-4c8b-b75e-6ee9be2ad040"
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "from zipfile import ZipFile\n",
    "!wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/app_data.zip\n",
    "\n",
    "# decompressing the data\n",
    "\n",
    "with ZipFile('app_data.zip', 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Data decompressed successfully')\n",
    "\n",
    "# removing the .zip file after extraction to clean space\n",
    "!rm app_data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a18X0kFGXUnt"
   },
   "source": [
    "# Loading the train/validation data, and re-scaling the labels to [0..1]\n",
    "- X_[train,valid,test] = Face images\n",
    "- Y_[train,valid,test] = Ground truth \n",
    "- M_[train,valid,test] = Metadata (gender, ethnicicy, facial expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GRhB1gzXlYc",
    "outputId": "c4aff0c1-eeaa-4331-90c6-4a87121e61cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loading the train data\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "M_train = np.load('./data/meta_data_train.npy')\n",
    "\n",
    "# loading the validation data\n",
    "X_valid = np.load('./data/data_valid.npy')\n",
    "Y_valid = np.load('./data/labels_valid.npy')\n",
    "M_valid = np.load('./data/meta_data_valid.npy')\n",
    "\n",
    "# loading the test data\n",
    "X_test = np.load('./data/data_test.npy')\n",
    "Y_test = np.load('./data/labels_test.npy')\n",
    "M_test = np.load('./data/meta_data_test.npy')\n",
    "\n",
    "# train labels are real numbers, ranging from ~0.9 to ~89 (years old);\n",
    "# we will re-scale the labels to [0,1] by using a normalization factor of 100,\n",
    "# assuming there is no sample with age > 100.\n",
    "Y_train = Y_train/100\n",
    "Y_valid = Y_valid/100\n",
    "# Y_test = Y_test/100 # -> we don't normalize the test labels as we will evaluate\n",
    "# them using the raw data, i.e., the apparent age values\n",
    "\n",
    "print('Train data size and shape', X_train.shape)\n",
    "print('Train labels size and shape', Y_train.shape)\n",
    "print('Train metadata size and shape', M_train.shape)\n",
    "print('----')\n",
    "print('Valid data size and shape', X_valid.shape)\n",
    "print('Valid labels size and shape', Y_valid.shape)\n",
    "print('Valid metadata size and shape', M_valid.shape)\n",
    "print('----')\n",
    "print('Test data size and shape', X_test.shape)\n",
    "print('Test labels size and shape', Y_test.shape)\n",
    "print('Test metadata size and shape', M_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVnh9fvUvoJ"
   },
   "source": [
    "# Visualizing some training samples\n",
    "Next, we multiply the normalized age labels by 100 to show the original age values on top of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "vxnI7M8s-JXf",
    "outputId": "fa3ce997-b041-44bd-bccd-612e9afbc15c"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = random.randint(0, len(X_train))\n",
    "    ax.imshow(cv2.cvtColor(X_train[idx, :, :, :], cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(Y_train[idx]*100)\n",
    "    ax.set(xlabel=[M_train[idx][0], M_train[idx][1], M_train[idx][2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDzps0ccD2pR"
   },
   "source": [
    "# Visualizing the age distribution of Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "jMCqsJtBD8KG",
    "outputId": "7e5c2cc9-cf59-4f6b-c7b6-324bd36a6c6b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Age distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# labels are multiplied by 100 to show the original values\n",
    "ax1.hist(Y_train*100, bins=50)\n",
    "ax1.set_title('Y_train labels')\n",
    "ax1.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax1.set_xlim([0, 100])\n",
    "\n",
    "ax2.hist(Y_valid*100, bins=50)\n",
    "ax2.set_title('Y_valid labels')\n",
    "ax2.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
    "ax2.set_xlim([0, 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uShj-2u-9HCD"
   },
   "source": [
    "# Visualizing the distributions of metadata (Train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "1SCU4nm5-qY2",
    "outputId": "b662e978-6da6-41f8-90e4-6a92ad7b4b02"
   },
   "outputs": [],
   "source": [
    "gender = []\n",
    "etnhicity = []\n",
    "emotion = []\n",
    "for sample in M_train:\n",
    "    gender.append(sample[0])\n",
    "    etnhicity.append(sample[1])\n",
    "    emotion.append(sample[2])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle('Metadata distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.hist(gender)\n",
    "ax2.hist(etnhicity)\n",
    "ax3.hist(emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNx5EZps5WN2"
   },
   "source": [
    "# Visualizing the age distribution per Ethnicity\n",
    "- First, we define a function to visualize the age distribution per ethnicity. Then, we visualize the distributions of train / validation / test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHNdQeyT8hSa"
   },
   "outputs": [],
   "source": [
    "def compute_hist_per_ethnicity(y_data, metadata, set):\n",
    "\n",
    "    vec_as = []\n",
    "    vec_af = []\n",
    "    vec_ca = []\n",
    "    for i in range(0, len(y_data)):\n",
    "        if(metadata[i][1] == 'asian'):\n",
    "            vec_as.append(y_data[i])\n",
    "        if(metadata[i][1] == 'afroamerican'):\n",
    "            vec_af.append(y_data[i])\n",
    "        if(metadata[i][1] == 'caucasian'):\n",
    "            vec_ca.append(y_data[i])\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    fig.suptitle(['Age distribution per Ethnicity ', set],\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax1.hist(vec_as, bins=50)\n",
    "    ax1.set_xlim([0, 100])\n",
    "    ax1.set(xlabel='Asian', ylabel='Num. of samples')\n",
    "\n",
    "    ax2.hist(vec_af, bins=50)\n",
    "    ax2.set_xlim([0, 100])\n",
    "    ax2.set(xlabel='Afroamerican', ylabel='Num. of samples')\n",
    "\n",
    "    ax3.hist(vec_ca, bins=50)\n",
    "    ax3.set_xlim([0, 100])\n",
    "    ax3.set(xlabel='Caucasian', ylabel='Num. of samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "5nqiYZMF9m4S",
    "outputId": "e03dce7f-ccea-4457-dc8f-44d8f1e7df15"
   },
   "outputs": [],
   "source": [
    "# train set\n",
    "compute_hist_per_ethnicity(Y_train*100, M_train, 'Train set')\n",
    "\n",
    "# validation set\n",
    "compute_hist_per_ethnicity(Y_valid*100, M_valid, 'Validation set')\n",
    "\n",
    "# test set\n",
    "# note, we do not multiply 'Y_test' by 100 because it was not normalized\n",
    "# to be in the range of [0,1] as the train and validation sets.\n",
    "compute_hist_per_ethnicity(Y_test, M_test, 'Test set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOc3kPMYzTCI"
   },
   "source": [
    "# Preprocessing the data (face images)\n",
    "- Later, we will define our model based on ResNet50 (our backbone). Originally,\n",
    "ResNet50 uses a regularization that changes the range of the input images. Thus,\n",
    "to be aligned with the ResNet50 input, we preprocess our input images using the respective 'preprocess_input' function. Later, if you decide to use another model as backbone (rather than ResNet), you may skip the following preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42bGw2eCzZ2r"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# train\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    x = X_train[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_train[i, ] = preprocess_input(x)\n",
    "\n",
    "# validation\n",
    "for i in range(0, X_valid.shape[0]):\n",
    "    x = X_valid[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_valid[i, ] = preprocess_input(x)\n",
    "\n",
    "# test\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    x = X_test[i, :, :, :]\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    X_test[i, ] = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGjZNPq_r9I7"
   },
   "source": [
    "# Downloading the ResNet50 model pre-trained on Faces\n",
    "We are using ResNet50 pre-trained on Faces (source [here](https://github.com/ox-vgg/vgg_face2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQwDAOner6rD",
    "outputId": "8de4e6ed-fe06-4f93-82a3-63b32ff50094"
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "!wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/model.zip\n",
    "\n",
    "# decompressing the data\n",
    "with ZipFile('model.zip', 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Model decompressed successfully')\n",
    "\n",
    "# removing the .zip file after extraction  to clean space\n",
    "!rm model.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a29243uAmxUS"
   },
   "source": [
    "# Loading the pre-trained model\n",
    "- You can see the data (e.g., we have downloaded) and structure of Colab by clicking on 'Files', on the left side <-- of this interface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J--yWIXom0uf",
    "outputId": "0d1e9b46-e392-406c-d550-0ce9771fb2c9"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# loading the pretrained model\n",
    "model = tf.keras.models.load_model('./model/weights.h5')\n",
    "\n",
    "# print the model summary\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrDpADCzzfjU"
   },
   "source": [
    "# Adapting the model to our needs\n",
    "- In summary, we will ignore the last layer 'classifier_low_dim' and will include a few other layers on top of our backbone. Here, we also define the activation function we are going to use as output of the last FC layer (Sigmoid, in the case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ABLExjBziRn",
    "outputId": "12179d4f-1b95-4ced-9017-a770b54cbdb8"
   },
   "outputs": [],
   "source": [
    "# Using the FC layer before the 'classifier_low_dim' layer as feature vector\n",
    "fc_512 = model.get_layer('dim_proj').output\n",
    "\n",
    "# adding a dropout layer to minimize overfiting problems\n",
    "dp_layer = Dropout(0.5)(fc_512)\n",
    "\n",
    "# adding a few hidden FC layers to learn hidden representations\n",
    "fc_64 = Dense(64, activation='relu', name='f_64')(dp_layer)\n",
    "\n",
    "# Includint an additional FC layer with sigmoid activation, used to regress\n",
    "# the apparent age\n",
    "output = Dense(1, activation='sigmoid', name='predict')(fc_64)\n",
    "\n",
    "# building and pringing the final model\n",
    "model = Model(inputs=model.get_layer('base_input').output,outputs=output)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_-Zh3Bt5Gx"
   },
   "source": [
    "# Freezing the first layers to allow the fine-tuning of the last FC layers (only)\n",
    "- Next, we set some layer to be trainable or not, and print if layers are set to trainable = True or False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pfL6NhouTe-",
    "outputId": "12bd4973-1380-409f-a4b0-e321f073ac5a"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for layer in model.layers:\n",
    "    if counter <= 174:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    #print(counter, layer.name, layer.trainable)\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM5L7P0EvVYn"
   },
   "source": [
    "# Printing the MODEL (summary) we have just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqipjTHzM6np",
    "outputId": "4190fa45-9018-41f8-ec60-3545ef1c0753"
   },
   "outputs": [],
   "source": [
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwG95hLURSGj"
   },
   "source": [
    "# IMPORTANT: Mounting your google drive to save your results\n",
    "- Colab gives you LIMITED GPU access. Thus, it may kill your process (of training) if you pass a limited amount of training hours. To allow you to save your model while training, you can mount your google drive, as detailed next. This way, if the process is killed, you can (in a new session) load your checkpoints (trained model, from your google drive) and, for example, continue training or make predictions with the model you obtained (even if trained for a few epochs).\n",
    "- In the following examples, the **the beset model (based on validation loss) is saved in my google drive inside a \"/temp/\" directory. You will need to addapt this path to your case.** \n",
    "- To save time, and to allow you to quickly 'play' and run the notebook, we have pre-trained some models, which are loaded (or not) based on some boolean variables (later, you will need to change/adapt these codes to achive the goals of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7sByyqKRae5",
    "outputId": "f6e70781-2e4b-46a4-f092-45af1a53ed56"
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "MOUNT_GOOGLE_DRIVE = False\n",
    "# --------------------------\n",
    "\n",
    "if(MOUNT_GOOGLE_DRIVE == True):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    # Note, the default path will be: '/content/gdrive/MyDrive/'\n",
    "    # In my case, the final path will be: '/content/gdrive/MyDrive/temp/' as I\n",
    "    # created a '/temp/' folder in my google drive for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n65k0Gc4vXRS"
   },
   "source": [
    "# Training the Model / or downloading a model already trained\n",
    "- As default, the code below will load a pre-trained model, obtained using the same code if LOAD_BEST_MODEL_ST1 is set to False.\n",
    "- Later, you can set LOAD_BEST_MODEL_ST1 to False to perfom the training.\n",
    "  - The code below uses Early stopping (es) with patience = 5 (that is, the training will stop if no improvement on valid_loss is observed on the last 5 epochs).\n",
    "  - It uses the Mean Squared Error (MSE) as loss function ('loss=tf.keras.losses.MeanSquaredError()'). The code also evaluates the Mean Absolute Error (MAE) during training ('metrics=['mae']'). Learning rate is set to 'learning_rate=1e-5', batch size = 32, and the model will be trained for 50 epochs (if Colab allows it based to the time budget)\n",
    "  - The model callback (mc) is set to save the best model based on valid_loss (that is, if validation loss decreases from one epoch to another, a new model is saved on the path you specify).\n",
    "  - Other hyperparameters you can play with are: defining another optimizer, loss function, learning rate, batch size, num of epochs.\n",
    "\n",
    "- Note: in case you want to save your model, stop training, and resume training, check the end of this file **\"II) illustrating how to train + save + stop training + RESUME TRAINING\"** where we provide a more detailed example about this procedure. Recommendation: first train your model for a few epochs to avoid the need of resume training. This way, you will get used with the code and the general pipeline. Later, you can play with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71Oh2HLcz2dg",
    "outputId": "88397546-09fc-44e8-cf6c-6a5eb137602c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle\n",
    "\n",
    "# load a model and train history (defined and trained\n",
    "# as below, trained for 38 epochs)\n",
    "# --------------------------\n",
    "LOAD_BEST_MODEL_ST1 = False  # (training only the last FC layers)\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    # downloading the trained model\n",
    "    !wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st1.zip\n",
    "    # decompressing the data\n",
    "    with ZipFile('best_model_st1.zip', 'r') as zip:\n",
    "        zip.extractall()\n",
    "        print('Model decompressed successfully')\n",
    "    # removing the .zip file after extraction  to clean space\n",
    "    !rm best_model_st1.zip\n",
    "\n",
    "else:\n",
    "    # defining the early stop criteria\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    # saving the best model based on val_loss\n",
    "    mc= ModelCheckpoint('./checkpoint/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    mc_2 = ModelCheckpoint('./checkpoint/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-6)\n",
    "\n",
    "    # defining the optimizer\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4, amsgrad = True), loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
    "\n",
    "    # training the model\n",
    "    history = model.fit(X_train, Y_train, validation_data=(\n",
    "        X_valid, Y_valid), batch_size=32, epochs=200, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n",
    "\n",
    "    # saving training history (for future visualization)\n",
    "    with open('./history/train_history.pkl', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U1hPksD9BYt"
   },
   "source": [
    "# Visualizing the train history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "xRKbcPYNOFRx",
    "outputId": "4d0655e6-b0c2-4ece-ba5d-05dcd398bb56"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# here, it loads the history of the model we have already trained, or loads the\n",
    "# history of the model you defined and trained\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    train_hist = pickle.load(open(\"./history/train_history.pkl\", \"rb\"))\n",
    "else:\n",
    "    train_hist = pickle.load(open(\"./history/train_history.pkl\", \"rb\"))\n",
    "\n",
    "# we plot both, the LOSS and MAE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Training history (stage 1)', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.plot(train_hist['loss'])\n",
    "ax1.plot(train_hist['val_loss'])\n",
    "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
    "ax1.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "ax2.plot(train_hist['mae'])\n",
    "ax2.plot(train_hist['val_mae'])\n",
    "ax2.set(xlabel='epoch', ylabel='MAE')\n",
    "ax2.legend(['train', 'valid'], loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDGOQjFz4k90"
   },
   "source": [
    "# Loading the saved model and Making predictions on the Test set\n",
    "- Next, we load the trained model and make predictions on the Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4OKbe-3GBMB",
    "outputId": "f7026058-4733-42d5-82ec-043b6530822b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_ST1 = True\n",
    "# --------------------------\n",
    "\n",
    "# loading the saved model\n",
    "if(LOAD_BEST_MODEL_ST1 == True):\n",
    "    saved_model = load_model('best_model.h5')\n",
    "else:\n",
    "    saved_model = load_model('./checkpoint/best_model.h5')\n",
    "\n",
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # predict on the test data\n",
    "    predictions_st1 = saved_model.predict(X_test, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3cP34WQI2pc"
   },
   "source": [
    "# Evaluating the model on the Test set, using the Mean Absolute Error (MAE) as metric.\n",
    "- Note, as the train/validation labels were re-scaled to be in the range of [0,1], the predictions will be in the same range [0,1]. \n",
    "- To evaluate the model on the test set (which was not normalized), we re-scale the predictions back using the normalization factor = 100 (previously defined), in order to have the Mean Absolute Error with respect to the original apparent age labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqdlJy5HGppZ",
    "outputId": "45f7460b-87d3-40ed-e653-1259faf6f11d"
   },
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_st1_f = predictions_st1*100\n",
    "\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(abs(np.subtract(predictions_st1_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPSOjBkSFpf3",
    "outputId": "9e500344-bdb1-4636-f805-385191609f63"
   },
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_ST1 == True):\n",
    "    # printing some predictions\n",
    "    for i in range(0, 10):\n",
    "        print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "              (predictions_st1_f[i], Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VITEsCYfHi6q"
   },
   "source": [
    "---\n",
    "# Performing a 2nd Stage of training, where ALL Layers are set to \"trainable\"\n",
    "- Up to here, we have just trained the last FC layers of our model. Now, we will load the model we have trained (referred to it as 1st stage), set all layers to TRAINABLE, and train the whole model. Training will take more time, but we expect to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0vM0ZCzH6_L",
    "outputId": "f45dfed7-53a4-44ca-e213-9ce268f0c46a"
   },
   "outputs": [],
   "source": [
    "# setting all layers of the model to trainable\n",
    "saved_model.trainable = True\n",
    "\n",
    "counter = 0\n",
    "for layer in saved_model.layers:\n",
    "    #print(counter, layer.name, layer.trainable)\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRrI2Vn9P_bw"
   },
   "source": [
    "# Training the WHOLE Model (2nd Stage)\n",
    "\n",
    "- As default, the code below will load a pre-trained model, obtained using the same code if LOAD_BEST_MODEL_ST2 is set to False.\n",
    "- Later, you can set LOAD_BEST_MODEL_ST2 to False to perfom the training.\n",
    "  - As before, the code below uses Early stopping (es) with patience = 5 (that is, the training will stop if no improvement on valid_loss is observed on the last 5 epochs).\n",
    "  - It uses the Mean Squared Error (MSE) as loss function ('loss=tf.keras.losses.MeanSquaredError()'). The code also evaluates the Mean Absolute Error (MAE) during training ('metrics=['mae']'). Learning rate is set to 'learning_rate=1e-5', batch size = 16, and the model will be trained for 12 epochs (if Colab allows it based to the time budget). Note, if you increase the batch size too much, data may not fit the GPU capacity (as the number of parameters to train increased compared to the 1st stage). This is why we reduced it from 32 to 16.\n",
    "  - The model callback (mc) is set to save the best model based on valid_loss (that is, if validation loss decreases from one epoch to another, a new model is saved on the path you specify).\n",
    "  - Other hyperparameters you can play with are: defining another optimizer, loss function, learning rate, batch size, num of epochs.\n",
    "- WARNING: at this stage, training take more time, and colab may close before you finish training due to time constraints. Thus, you will need to define a good strategy! In case you want to save your model, stop training, and resume training, check the end of this file **\"II) illustrating how to train + save + stop training + RESUME TRAINING\"** where we provide a more detailed example about this procedure.\n",
    "- WARNING: if you save your model and resume training, the train history will be lost. To monitore the training history, you may need to save the train history in another way (e.g., you can copy and paste the logs into a text file before resuming the training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXQZXojAxGJc"
   },
   "source": [
    "---\n",
    "---\n",
    "# Accuracy is not enough! We also need to evaluate how biased is our model!\n",
    "- Next, we define different different functions, used to compute a bias score given different attributes.\n",
    "  - Age bias\n",
    "  - Gender bias\n",
    "  - Ethnicity bias\n",
    "  - Facial Expression bias\n",
    "- In a nutshell, given a particular attribute, we compute the MAE for different groups. For the case of age, detailed next, we will have 4 groups base on different age ranges. Then, we will have $MAE_1$, $MAE_2$, $MAE_3$ and $MAE_4$. Then, we compute the Absolute Difference among all. That is,\n",
    "  - $D_{1,2} = |MAE_1-MAE_2|$\n",
    "  - $D_{1,3} = |MAE_1-MAE_3|$\n",
    "  - $D_{1,4} = |MAE_1-MAE_4|$\n",
    "  - $D_{2,3} = |MAE_2-MAE_3|$\n",
    "  - $D_{2,4} = |MAE_2-MAE_4|$\n",
    "  - $D_{3,4} = |MAE_3-MAE_4|$\n",
    "\n",
    "- The final score is obtained by the average of the absolute differentes. In the case of age:\n",
    "  - $B_a = (D_{1,2} + D_{1,3} + D_{1,4} + D_{2,3} + D_{2,4} + D_{3,4})/6$\n",
    "\n",
    "- To minimize your bias score, given a particular attribute, you will need to minimize the Absolute Difference among the different groups being evaluated.\n",
    "- The big challenge here is to minimize ALL bias scores (i.e., age, gender, ethnicity and face expression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlPY4wbeUr_7"
   },
   "source": [
    "---\n",
    "# Age Bias ($B_a$) \n",
    "\n",
    "- Evaluates (on the TEST set) how accurate the model is with respect to different age ranges.\n",
    "  - group 1: age < 20\n",
    "  - group 2: 20 <= age < 40\n",
    "  - group 3: 40 <= age < 60\n",
    "  - group 4: 60 <= age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umMOd4OpWP5y"
   },
   "outputs": [],
   "source": [
    "def age_bias(predictions, gt):\n",
    "    error_g1 = []\n",
    "    error_g2 = []\n",
    "    error_g3 = []\n",
    "    error_g4 = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(gt[i] < 20):\n",
    "            error_g1.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 20 and gt[i] < 40):\n",
    "            error_g2.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 40 and gt[i] < 60):\n",
    "            error_g3.append(abs(predictions[i]-gt[i]))\n",
    "        if(gt[i] >= 60):\n",
    "            error_g4.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Age analysis:')\n",
    "    print('Size group 1 = %d, MAE = %f' % (len(error_g1), np.mean(error_g1)))\n",
    "    print('Size group 2 = %d, MAE = %f' % (len(error_g2), np.mean(error_g2)))\n",
    "    print('Size group 3 = %d, MAE = %f' % (len(error_g3), np.mean(error_g3)))\n",
    "    print('Size group 4 = %d, MAE = %f' % (len(error_g4), np.mean(error_g4)))\n",
    "\n",
    "    age_bias = (abs(np.mean(error_g1)-np.mean(error_g2)) +\n",
    "                abs(np.mean(error_g1)-np.mean(error_g3)) +\n",
    "                abs(np.mean(error_g1)-np.mean(error_g4)) +\n",
    "                abs(np.mean(error_g2)-np.mean(error_g3)) +\n",
    "                abs(np.mean(error_g2)-np.mean(error_g4)) +\n",
    "                abs(np.mean(error_g3)-np.mean(error_g4)))/6\n",
    "\n",
    "    print('---------')\n",
    "    print('Age bias (Ba) = ', age_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALWjE5J8edq7"
   },
   "source": [
    "# Gender Bias ($B_g$) \n",
    "- Evaluates (on the test set) how accurate the model is with respect to different gender.\n",
    "  - group 1: male\n",
    "  - group 2: female\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ua8JzFfFZK"
   },
   "outputs": [],
   "source": [
    "def gender_bias(predictions, gt, metadata):\n",
    "    error_m = []\n",
    "    error_f = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][0] == 'female'):\n",
    "            error_f.append(abs(predictions[i]-gt[i]))\n",
    "        else:\n",
    "            error_m.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Gender analysis:')\n",
    "    print('Size group female = %d, MAE = %f' %\n",
    "          (len(error_f), np.mean(error_f)))\n",
    "    print('Size group male = %d, MAE = %f' % (len(error_m), np.mean(error_m)))\n",
    "\n",
    "    gender_bias = abs(np.mean(error_f)-np.mean(error_m))\n",
    "\n",
    "    print('---------')\n",
    "    print('Gender bias (Bg) = ', gender_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8rIPUIrjkDG"
   },
   "source": [
    "# Ethnicity Bias ($B_e$)\n",
    "- Evaluates (on the test set) how accurate the model is with respect to different ethnicity categories.\n",
    "  - group 1: asian\n",
    "  - group 2: afroamerican\n",
    "  - group 3: caucasian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BnG-wK2j8nA"
   },
   "outputs": [],
   "source": [
    "def ethnicity_bias(predictions, gt, metadata):\n",
    "    error_as = []\n",
    "    error_af = []\n",
    "    error_ca = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][1] == 'asian'):\n",
    "            error_as.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][1] == 'afroamerican'):\n",
    "            error_af.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][1] == 'caucasian'):\n",
    "            error_ca.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Ethnicity Analysis:')\n",
    "    print('Size group asian = %d, MAE = %f' %\n",
    "          (len(error_as), np.mean(error_as)))\n",
    "    print('Size group afroamerican = %d, MAE = %f' %\n",
    "          (len(error_af), np.mean(error_af)))\n",
    "    print('Size group caucasian = %d, MAE = %f' %\n",
    "          (len(error_ca), np.mean(error_ca)))\n",
    "\n",
    "    ethnicity_bias = (abs(np.mean(error_as)-np.mean(error_af)) +\n",
    "                      abs(np.mean(error_as)-np.mean(error_ca)) +\n",
    "                      abs(np.mean(error_af)-np.mean(error_ca)))/3\n",
    "\n",
    "    print('---------')\n",
    "    print('Ethnicity bias (Be) = ', ethnicity_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohPkAMvxm9Ed"
   },
   "source": [
    "# Face expression bias ($B_f$)\n",
    "- Evaluates (on the test set) how accurate the model is with respect to different face expression categories.\n",
    "  - group 1: neutral\n",
    "  - group 2: slightlyhappy\n",
    "  - group 3: happy\n",
    "  - group 4: other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dJOGVMGm_Fe"
   },
   "outputs": [],
   "source": [
    "def face_expression_bias(predictions, gt, metadata):\n",
    "    error_h = []\n",
    "    error_s = []\n",
    "    error_n = []\n",
    "    error_o = []\n",
    "    for i in range(0, len(gt)):\n",
    "        if(metadata[i][2] == 'happy'):\n",
    "            error_h.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'slightlyhappy'):\n",
    "            error_s.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'neutral'):\n",
    "            error_n.append(abs(predictions[i]-gt[i]))\n",
    "        if(metadata[i][2] == 'other'):\n",
    "            error_o.append(abs(predictions[i]-gt[i]))\n",
    "\n",
    "    print('=============================')\n",
    "    print('Face experession Analysis:')\n",
    "    print('Size group happy = %d, MAE = %f' % (len(error_h), np.mean(error_h)))\n",
    "    print('Size group slightlyhappy = %d, MAE = %f' %\n",
    "          (len(error_s), np.mean(error_s)))\n",
    "    print('Size group neutral = %d, MAE = %f' %\n",
    "          (len(error_n), np.mean(error_n)))\n",
    "    print('Size group other = %d, MAE = %f' % (len(error_o), np.mean(error_o)))\n",
    "\n",
    "    face_bias = (abs(np.mean(error_h)-np.mean(error_s)) +\n",
    "                 abs(np.mean(error_h)-np.mean(error_n)) +\n",
    "                 abs(np.mean(error_h)-np.mean(error_o)) +\n",
    "                 abs(np.mean(error_s)-np.mean(error_n)) +\n",
    "                 abs(np.mean(error_s)-np.mean(error_o)) +\n",
    "                 abs(np.mean(error_n)-np.mean(error_o)))/6\n",
    "\n",
    "    print('---------')\n",
    "    print('Face Expression bias (Bf) = ', face_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjkJNXGilNfo"
   },
   "source": [
    "---\n",
    "---\n",
    "# Strategies to improve Accuracy (i.e., to reduce the Error):\n",
    "# 2) Custom Loss: sample weights to deal with inbalanced categories\n",
    "- Next, we will created a \"customized loss\", which gives more weight to people having less samples in train data. For this, **we will consider the age range only**. This way, we believe the model will be able to generalize a little bit better to those particular groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGjU8Xetxl3W"
   },
   "source": [
    "# Load the Train data again (to remove the augmented data) and generate the weigths \n",
    "- First, we will generate a weight for each age group (for g =1 to 4);\n",
    "- The formula used to calculate the weight for each group $j$ is:\n",
    "\n",
    "  $w_j=n_{samples} / (n_{classes} * n_{samples,j}),$\n",
    "\n",
    "  Where\n",
    "\n",
    "    - $w_j$ is the weight for each group $j$,\n",
    "    - $n_{samples}$ is the number of samples in the train set,\n",
    "    - $n_{classes}$ is the number of classes (4 in our case, as we divided the ages in 4 groups),\n",
    "    - $n_{samples,j}$ is the number of samples of class (group) $j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = 0\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "      g1 +=1\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "      g2 +=1\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "      g3 +=1\n",
    "    if(Y_train[i]*100>=60):\n",
    "      g4 +=1\n",
    "print('group(s) size = ', [g1, g2, g3, g4])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "      sample_weights.append(w[0])\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "      sample_weights.append(w[1])\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "      sample_weights.append(w[2])\n",
    "    if(Y_train[i]*100>=60):\n",
    "      sample_weights.append(w[3])\n",
    "sample_weights = np.array(sample_weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svMMpvU9xcJR",
    "outputId": "35314142-5999-4079-a06c-f197cdfb2bf9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "M_train = np.load('./data/meta_data_train.npy')\n",
    "Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = g5 = g6 = g7 = g8 = g9 = g10 = g11 = g12 = g13 = g14 = g15 = g16 = g17 = g18 = g19 = g20 = g21 = g22 = g23 = g24 = 1\n",
    "g25 = g26 = g27 = g28 = g29 = g30 = g31 = g32 = g33 = g34 = g35 = g36 = g37 = g38 = g39 = g40 = 1\n",
    "g41 = g42 = g43 = g44 = g45 = g46 = g47 = g48 = 1\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g1 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g2 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g3 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g4 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g5 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g6 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g7 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g8 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g9 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g10 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g11 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g12 +=1\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g13 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g14 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g15 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g16 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g17 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g18 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g19 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g20 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g21 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g22 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g23 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g24 +=1\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g25 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g26 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g27 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g28 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g29 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g30 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g31 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g32 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g33 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g34 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g35 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g36 +=1\n",
    "    if(Y_train[i]*100>=60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g37 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g38 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g39 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g40 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g41 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g42 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g43 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g44 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                g45 +=1\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                g46 +=1\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                g47 +=1\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                g48 +=1\n",
    "print('group(s) size = ', [g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10 , g11 , g12 , g13 , g14 , g15 , g16 , g17 , g18 , g19 , g20 , g21 , g22 , g23 , g24 , \n",
    "g25 , g26 , g27 , g28 , g29 , g30 , g31 , g32 , g33 , g34 , g35 , g36 , g37 , g38 , g39 , g40 , \n",
    "g41 , g42 , g43 , g44 , g45 , g46 , g47 , g48])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10 , g11 , g12 , g13 , g14 , g15 , g16 , g17 , g18 , g19 , g20 , g21 , g22 , g23 , g24 , \n",
    "g25 , g26 , g27 , g28 , g29 , g30 , g31 , g32 , g33 , g34 , g35 , g36 , g37 , g38 , g39 , g40 , \n",
    "g41 , g42 , g43 , g44 , g45 , g46 , g47 , g48]))/(48*np.array([g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10 , g11 , g12 , g13 , g14 , g15 , g16 , g17 , g18 , g19 , g20 , g21 , g22 , g23 , g24 , \n",
    "g25 , g26 , g27 , g28 , g29 , g30 , g31 , g32 , g33 , g34 , g35 , g36 , g37 , g38 , g39 , g40 , \n",
    "g41 , g42 , g43 , g44 , g45 , g46 , g47 , g48]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[0])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[1])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[2])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[3])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[4])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[5])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[6])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[7])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[8])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[9])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[10])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[11])\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[12])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[13])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[14])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[15])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[16])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[17])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[18])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[19])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[20])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[21])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[22])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[23])\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[24])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[25])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[26])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[27])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[28])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[29])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[30])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[31])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[32])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[33])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[34])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[35])\n",
    "    if(Y_train[i]*100>=60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[36])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[37])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[38])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[39])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[40])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[41])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[42])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[43])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            if(M_train[i][2] == 'neutral'):\n",
    "                sample_weights.append(w[44])\n",
    "            if(M_train[i][2] == 'slightlyhappy'):\n",
    "                sample_weights.append(w[45])\n",
    "            if(M_train[i][2] == 'happy'):\n",
    "                sample_weights.append(w[46])\n",
    "            if(M_train[i][2] == 'other'):\n",
    "                sample_weights.append(w[47])\n",
    "sample_weights = np.array(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# loading the train data again (original face images, before preprocessing):\n",
    "X_train = np.load('./data/data_train.npy')\n",
    "Y_train = np.load('./data/labels_train.npy')\n",
    "M_train = np.load('./data/meta_data_train.npy')\n",
    "Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
    "\n",
    "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
    "for i in range(0,X_train.shape[0]):\n",
    "  x = X_train[i,:,:,:]\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  X_train[i,] = preprocess_input(x)\n",
    "\n",
    "# counting the number of samples per group in the train data (age attribute only)\n",
    "g1 = g2 = g3 = g4 = g5 = g6 = g7 = g8 = g9 = g10 = 0\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            g1 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            g2 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            g3 +=1\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            g4 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            g5 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            g6 +=1\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            g7 +=1\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            g8 +=1\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            g9 +=1\n",
    "    if(Y_train[i]*100>=60):\n",
    "        g10 +=1\n",
    "                \n",
    "print('group(s) size = ', [g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10])\n",
    "\n",
    "# generating the weights for each group using the equation defined above\n",
    "w = sum(np.array([g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10]))/(10*np.array([g1 , g2 , g3 , g4 , g5 , g6 , g7 , g8 , g9 , g10]))\n",
    "print('weights per group = ', w)\n",
    "\n",
    "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
    "sample_weights = []\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i]*100<20):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            sample_weights.append(w[0])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            sample_weights.append(w[1])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            sample_weights.append(w[2])\n",
    "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            sample_weights.append(w[3])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            sample_weights.append(w[4]) \n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            sample_weights.append(w[5])\n",
    "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
    "        if(M_train[i][1]=='caucasian'):\n",
    "            sample_weights.append(w[6])\n",
    "        if(M_train[i][1]=='asian'):\n",
    "            sample_weights.append(w[7])\n",
    "        if(M_train[i][1]=='afroamerican'):\n",
    "            sample_weights.append(w[8])\n",
    "    if(Y_train[i]*100>=60):\n",
    "        sample_weights.append(w[9])\n",
    "sample_weights = np.array(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gTtMXGmx5jA"
   },
   "source": [
    "# I) Using the SAMPLE WEIGHTS to train our model and,\n",
    "- Next, you will see the code we used to train our model (2nd stage) from the model we obtained at the 1st stage, using the customized loss option with sample weights.\n",
    "- As default, the code will load the model already trained. \n",
    "- You can change the boolean variable 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS' to False to train your model.\n",
    "- Note, now we include other variables ('RESUME_TRAINING' and 'RESUME_FROM_EPOCH') to allow us resuming training, as well as to inform from what epoch we want to resume the trainind, detailed below.\n",
    "\n",
    "# II) illustrating how to train + save + stop training + RESUME TRAINING\n",
    "- **Imagine** you set 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = False', 'NUM_EPOCHS = 12' and 'RESUME_TRAINING = False' to train your model the first time.\n",
    "- Due to Colab limitations, your process stoped the training at the middle of epoch 10, and you saved the best model based on validation loss on epoch 9.\n",
    "- In the above example, you can resume training from epoch 9 by setting the following parameters:\n",
    "  - 'RESUME_TRAINING = True'\n",
    "  - 'RESUME_FROM_EPOCH = 9'\n",
    "\n",
    "- IMPORTANT: to resume training, you will need to monitor the epoch number where your model stopped before resuming the training, and change the defined variables properly. \n",
    "  - Note that the fit function is adapted to receive the sample weights ('sample_weight=sample_weights').\n",
    "  - Also note that the fit function changes if you are training from epoch 0 (initial_epoch=0) or resume training (initial_epoch=RESUME_FROM_EPOCH). \n",
    "  - Finally, note that when you are resuming training, you load your 'best_model_2nd_stage_weighted.h5' instead of the model trained at stage 1 ('best_model.h5').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjEx3B3lx8Mu",
    "outputId": "fc038ac4-299d-4680-fc7e-06a1cad294aa"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "# --------------------------\n",
    "LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = False\n",
    "NUM_EPOCHS = 300\n",
    "# --------------------------\n",
    "RESUME_TRAINING = False\n",
    "RESUME_FROM_EPOCH = 90\n",
    "# --------------------------\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    # downloading the trained model\n",
    "    !wget https: // data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_weighted.zip\n",
    "    # decompressing the data\n",
    "    with ZipFile('best_model_weighted.zip', 'r') as zip:\n",
    "        zip.extractall()\n",
    "        print('Model decompressed successfully')\n",
    "    # removing the .zip file after extraction  to clean space\n",
    "    !rm best_model_weighted.zip\n",
    "\n",
    "else:\n",
    "    # loading the saved model (best model learned at stage 1)\n",
    "    if(RESUME_TRAINING == False):\n",
    "        # load model from stage 1 best_model\n",
    "        saved_model = load_model('./checkpoint/best_model.h5')\n",
    "    else:\n",
    "        # resume training (stage 2)\n",
    "        saved_model = load_model('./checkpoint/best_model.h5')\n",
    "\n",
    "    # setting all layers to traineble\n",
    "    saved_model.trainable = True\n",
    "\n",
    "    # =================================================\n",
    "    # training all layers (2nd stage), given the model saved on stage 1\n",
    "    saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5, amsgrad=False), loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
    "    # =================================================\n",
    "\n",
    "    # defining the early stop criteria\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "    mc = ModelCheckpoint('./checkpoint/best_model_2nd_stage_weighted.h5',\n",
    "                         monitor='val_loss', mode='min', save_best_only=True)\n",
    "    mc_2 = ModelCheckpoint('./checkpoint/best_model_2nd_stage_weighted_mae.h5',\n",
    "                         monitor='val_loss', mode='min', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6, min_lr=1e-6)\n",
    "\n",
    "\n",
    "    if(RESUME_TRAINING == False):\n",
    "        history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(\n",
    "            X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n",
    "    else:\n",
    "        history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(\n",
    "            X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es, mc, mc_2, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# here, it loads the history of the model we have already trained, or loads the\n",
    "# history of the model you defined and trained\n",
    "\n",
    "train_hist = history.history\n",
    "\n",
    "# we plot both, the LOSS and MAE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "fig.suptitle('Training history (stage 1)', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax1.plot(train_hist['loss'])\n",
    "ax1.plot(train_hist['val_loss'])\n",
    "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
    "ax1.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "ax2.plot(train_hist['mae'])\n",
    "ax2.plot(train_hist['val_mae'])\n",
    "ax2.set(xlabel='epoch', ylabel='MAE')\n",
    "ax2.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7kqieNu7zDS"
   },
   "source": [
    "# Making predictions on the Test set and Evaluating\n",
    "- Note that in this case, the model obtained MAE = 12.45038828, which is not the best score compared to the ones obtained before. However, are the evaluated biases better? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYq1tCet70bN",
    "outputId": "3111c657-af7c-4de6-9152-c34edb81075c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    saved_model_2nd_weighted = load_model('best_model_2nd_stage_weighted.h5')\n",
    "else:\n",
    "    saved_model_2nd_weighted = load_model('./checkpoint/best_model_2nd_stage_weighted.h5')\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_WEIGHTED = True\n",
    "# --------------------------\n",
    "\n",
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # predict on the test data\n",
    "    predictions_2nd_weighted = saved_model_2nd_weighted.predict(\n",
    "        X_test, batch_size=32, verbose=1)\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_2nd_weighted_f = predictions_2nd_weighted*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCyMYVPgILxq",
    "outputId": "d5a3f622-8de6-42e2-9c68-243288f54588"
   },
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(\n",
    "            abs(np.subtract(predictions_2nd_weighted_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RD-J3swDIjQW",
    "outputId": "c011877a-a70c-45a9-8e05-5cfc1ba17d98"
   },
   "outputs": [],
   "source": [
    "# printing some predictions\n",
    "for i in range(0, 20):\n",
    "    print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "          (predictions_2nd_weighted_f[i], Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_bias(predictions_2nd_weighted_f, Y_test)\n",
    "\n",
    "# computing the gender bias (model_stage_2)\n",
    "gender_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "# computing the ethnicity bias (model_stage_2)\n",
    "ethnicity_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "# computing the face bias (model_stage_2)\n",
    "face_expression_bias(predictions_2nd_weighted_f, Y_test, M_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# saving the predictions as a csv file\n",
    "with open('predictions.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(predictions_2nd_weighted_f)\n",
    "    csvFile.close()\n",
    "\n",
    "# compressing the csv file (to be submitted to codalab as prediction)\n",
    "! zip predictions.zip predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS == True):\n",
    "    saved_model_2nd_weighted = load_model('best_model_2nd_stage_weighted_mae.h5')\n",
    "else:\n",
    "    saved_model_2nd_weighted = load_model('./checkpoint/best_model_2nd_stage_weighted_mae.h5')\n",
    "\n",
    "# --------------------------\n",
    "ENABLE_EVALUATION_WEIGHTED = True\n",
    "# --------------------------\n",
    "\n",
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # predict on the test data\n",
    "    predictions_2nd_weighted = saved_model_2nd_weighted.predict(\n",
    "        X_test, batch_size=32, verbose=1)\n",
    "    # re-scaling the output predictions (from [0,1] to age range) using the\n",
    "    # the normalization factor mentioned before\n",
    "    predictions_2nd_weighted_f = predictions_2nd_weighted*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ENABLE_EVALUATION_WEIGHTED == True):\n",
    "    # evaluating on test data\n",
    "    error = []\n",
    "    for i in range(0, len(Y_test)):\n",
    "        error.append(\n",
    "            abs(np.subtract(predictions_2nd_weighted_f[i][0], Y_test[i])))\n",
    "\n",
    "    print('MAE = %.8f' % (np.mean(error)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing some predictions\n",
    "for i in range(0, 20):\n",
    "    print('predicted age = %.3f - Ground truth = %.3f' %\n",
    "          (predictions_2nd_weighted_f[i], Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpJCmvGQXDIS"
   },
   "source": [
    "# Comparing the 2nd stage of training: \n",
    "case a) without augmentation and custom loss **VS.** case b) without augmentation but with custom loss.\n",
    "- Age bias:\n",
    "  - case a: 8.988896687825521\n",
    "  - case b: 3.965182622273763\n",
    "- Gender bias:\n",
    "  - case a: 0.6280031\n",
    "  - case b: 0.54932594\n",
    "- Ethnicity bias:\n",
    "  - case a: 2.447519620259603\n",
    "  - case b: 2.094111124674479\n",
    "- Face Expression bias:\n",
    "  - case a: 0.8196892738342285\n",
    "  - case b: 1.220861275990804\n",
    "\n",
    "As it can be observed, the model with custom loss and weighted samples obtained overall smaller bias scores on all evaluated attributes (except for face expression), even if the weigts were defined based on age attribute only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjrDG9AHLoPf",
    "outputId": "ca7130e8-a745-4ace-ca85-9067cae44bb3"
   },
   "outputs": [],
   "source": [
    "age_bias(predictions_2nd_f, Y_test)\n",
    "age_bias(predictions_2nd_weighted_f, Y_test)\n",
    "\n",
    "gender_bias(predictions_2nd_f, Y_test, M_test)\n",
    "gender_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "ethnicity_bias(predictions_2nd_f, Y_test, M_test)\n",
    "ethnicity_bias(predictions_2nd_weighted_f, Y_test, M_test)\n",
    "\n",
    "face_expression_bias(predictions_2nd_f, Y_test, M_test)\n",
    "face_expression_bias(predictions_2nd_weighted_f, Y_test, M_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# saving the predictions as a csv file\n",
    "with open('predictions.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(predictions_2nd_weighted_f)\n",
    "    csvFile.close()\n",
    "\n",
    "# compressing the csv file (to be submitted to codalab as prediction)\n",
    "! zip predictions_2.zip predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-sTrrsB4oR6"
   },
   "source": [
    "---\n",
    "---\n",
    "# Practical Exercises \n",
    "Next, we define a serie of practical exercises (Task 1 and 2, and an optional extra exercise).  **Your goal is to maximize accuracy (i.e., reduce the Mean Absolute Error) and minimize the evaluated bias scores on the different attributes**. Task 1 and 2 have some restrictions so that you can compare the results when following different strategies. Note, you can edit and improve the starting kit on each task, but you are free to start from strach and create a new solution. At the end, you will be evaluated based on a set of items (detailed in the practical classes) and **creativity**. \n",
    "\n",
    "- IMPORTANT: we will use **Codalab** to motivate the students, as they can submit their results on the platform, compete with each other and improve their solutions, but the ranking shown in the leaderboard will not be considered in the evaluation. This is to justify that more creative solutions will be priefered even if they don't provide the best results.\n",
    "- Note: you will be requested to share with the lectors (Sergio and Julio) your final **Colab file** (with a clean code and well documented) and a **Report document** where you describe your experiments and solution, compare and discuss the obtained results in a progressive and clear way. Please, check the class material associated to the practical sessions for more details. \n",
    "\n",
    "---\n",
    "- **Task 1 (with data augmentation):** For this task, you should define your model (e.g., a generic backbone with some small changes to solve the problem at hand, like including/removing some layers, etc), play with the different hyperparameters (e.g., number of epochs, learning rate, batch size, etc), regularizers (e.g., dropout layer), loos function (e.g., MSE, MAE, etc). You can also play with the training strategy (e.g., training using different stages - or not - freezing different layers during training - or not, etc). Then, **you will be requested to perform some data augmentation** to achieve your goal. Note, you could simply expand the idea of the starting kit to cover other attributes (e.g., age > 40 or for \"happy\" expression). However, we expect more creative solutions, where different approaches are employed (e.g., new transformations, covering different attributes, etc). Then, you should submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "\n",
    "- **Task 2 (custom loss, without data augmentation):** For this task, you should fix the model employed in Task 1, but you can also play with the different hyperparameters, regularizers, loos function and training strategy. Then, **you will be requested to use a custom loss (e.g., sample weights)** to achieve your goal, **without any data augmentation** method. This way, you will be able to compare the different solutions (Task 1 vs. Task 2). Then, you should submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "\n",
    "- **Extra (optional):** For this task, you can exploit your creativity as much as you can. You are free to employ any strategy, data augmentation, custom loss, etc, all together in order to achieve your goal. Then, you will be able to compare the obtained results for Task 1 vs. Task 2 vs. the extra (optional) task. Then, you can also submit your solution to codalab and receive real-time feedback, and improve it based on your results.\n",
    "---\n",
    "\n",
    "\n",
    "**Codalab Competition link:** https://codalab.lisn.upsaclay.fr/competitions/2321?secret_key=b66c95cb-997c-4fc9-af4e-987721abfa6c \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dG2MLbQvZy9h",
    "XwmfFS5oPbhn",
    "BXQZXojAxGJc",
    "_UY-ci6Yc7VR",
    "xjkJNXGilNfo",
    "F-sTrrsB4oR6"
   ],
   "name": "2022_UB-Master_Colab_introduction_age.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
