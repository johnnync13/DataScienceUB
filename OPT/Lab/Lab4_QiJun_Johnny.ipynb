{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P4_Optimitzacio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRvBvlW0vdVr"
      },
      "source": [
        "# Constrained optimization: equality constraints\n",
        "### By Qijun Jin and Johnny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P7UX_Thvq4E"
      },
      "source": [
        "### Sequential Quadratic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8GhrwjQx62Z"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqVhIA04x-22"
      },
      "source": [
        "def f(x):\n",
        "    return np.array([np.power(np.e, 3 * x[0]) + np.power(np.e, -4 * x[1])], dtype=np.float64)\n",
        "\n",
        "\n",
        "def h(x):\n",
        "    return np.array([np.power(x[0], 2) + np.power(x[1], 2) - 1], dtype=np.float64)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNPOWK7Nx_aO"
      },
      "source": [
        "def df(x):\n",
        "    d1 = 3 * np.power(np.e, 3 * x[0])\n",
        "    d2 = -4 * np.power(np.e, -4 * x[1])\n",
        "    return np.array([d1, d2])\n",
        "\n",
        "\n",
        "def dh(x):\n",
        "    d1 = 2 * x[0]\n",
        "    d2 = 2 * x[1]\n",
        "    return np.array([d1, d2])\n",
        "\n",
        "\n",
        "def dl(x, lamb):\n",
        "    return df(x) - lamb * dh(x)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Ttjw-zyDDn"
      },
      "source": [
        "def d2f(x):\n",
        "    d11 = 9 * np.power(np.e, 3 * x[0])\n",
        "    d12 = 0\n",
        "    d21 = 0\n",
        "    d22 = 16 * np.power(np.e, -4 * x[1])\n",
        "    return np.array([[d11, d12], [d21, d22]])\n",
        "\n",
        "\n",
        "def d2h(x):\n",
        "    d11 = 2\n",
        "    d12 = 0\n",
        "    d21 = 0\n",
        "    d22 = 2\n",
        "    return np.array([[d11, d12], [d21, d22]])\n",
        "\n",
        "\n",
        "def d2l(x):\n",
        "    return d2f(x) - lamb * d2h(x)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRUqqS0VyHC-"
      },
      "source": [
        "def d2la(x, lamb):\n",
        "    G = d2f(x) - lamb * d2h(x)\n",
        "    C = -dh(x)\n",
        "\n",
        "    matrix = np.zeros((3, 3))\n",
        "    matrix[:2, :2] = G\n",
        "    matrix[-1, :2] = C\n",
        "    matrix[:2, -1] = C\n",
        "    \n",
        "    return matrix"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXDSCmLIzC9g"
      },
      "source": [
        "###### 1. The stopping criterion we have estalished is to check that the norm of $\\nabla xL$ is major than epsilon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx36pkR7bLdt"
      },
      "source": [
        "def solve(x, lamb, max_iter=1000, eps = 10e-16):\n",
        "    i = 0\n",
        "\n",
        "    dlx = dl(x, lamb)\n",
        "\n",
        "    while i < max_iter and np.linalg.norm(dlx) > eps:\n",
        "        A = d2la(x, lamb)\n",
        "        dlx = dl(x, lamb)\n",
        "        b = np.concatenate((-dlx, h(x)))\n",
        "        d = np.linalg.solve(A, b)\n",
        "\n",
        "        x += d[:-1]\n",
        "        lamb += d[-1]\n",
        "        i += 1\n",
        "\n",
        "    return x, lamb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmqoZETj-EKW",
        "outputId": "2c61d4f2-19c7-461e-bc86-676b4ab99953"
      },
      "source": [
        "x = np.array([-1., 1.])\n",
        "\n",
        "lamb = -1\n",
        "\n",
        "print(solve(x, lamb))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.74833549,  0.66332043]), -0.21232493554997134)\n",
            "(array([-0.74833549,  0.66332043]), -0.2123249355499713)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudotFHM-Q_r"
      },
      "source": [
        "###### 1. We can can see that the algorithm with initial point closed to the minimum can be converged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPRx2FRyQa4"
      },
      "source": [
        "def merit(x, p=10):\n",
        "    return f(x) + p * np.power(h(x), 2)\n",
        "\n",
        "def dmerit(x, p=10):\n",
        "    d1 = df(x)\n",
        "    d2 = 2 * p * dh(x)\n",
        "    return d1 + d2"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJG1Jhr0kOa"
      },
      "source": [
        "###### 2. Setting x = (100, 100)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cQehktj0m9e",
        "outputId": "f902b6c9-eefb-4ea5-8715-e5680733f3de"
      },
      "source": [
        "x = np.array([100., 100.])\n",
        "\n",
        "lamb = -1\n",
        "\n",
        "print(solve(x, lamb))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([nan, nan]), nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in power\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in power\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVn-NPBq-qZC"
      },
      "source": [
        "###### 2. We can see that with initial point far away from minimum pont cannot be solved by Newton-like iterarion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phXYKfn1E1G"
      },
      "source": [
        "###### 3. Implement gradient descent method with Merit function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LylQmEOyR-V"
      },
      "source": [
        "def gradient_descend_2d(dmerit, x, alpha=1, eps=1e-6, max_iter=1000000):\n",
        "    i = 0\n",
        "\n",
        "    while i < max_iter:\n",
        "        dm = dmerit(x)\n",
        "        dm_norm = np.linalg.norm(dm)\n",
        "        dm_normalized = dm / dm_norm\n",
        "\n",
        "        x -= alpha * dm_normalized\n",
        "        i += 1\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thjxbo4w_BXY"
      },
      "source": [
        "###### In order to find the minimum, we have used the gradient descent method. In this gradient descent methods, we have penalized the constraint function quadratically when the initial point is far away from the minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCg0x5251yOa"
      },
      "source": [
        "###### 4. Gradient descent and Newton-like iteration with Merit function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rjo0MkiyTLB",
        "outputId": "5757be1c-2dc1-432b-c245-98feb5cf8d26"
      },
      "source": [
        "x = np.array([100., 100.])\n",
        "\n",
        "minimum = gradient_descend_2d(dmerit, x, max_iter=100)\n",
        "\n",
        "print(minimum)\n",
        "\n",
        "lamb = -1\n",
        "\n",
        "print(solve(minimum, lamb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.55672317 97.89137348]\n",
            "(array([-0.74833549,  0.66332043]), -0.2123249355499713)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyo8R9bD_JMG"
      },
      "source": [
        "###### As we can see that if we applied first the gradient descent method with the minimizers of the constrained problem. We can approach to a point which is 'optimal' for the Newton-like iteration to find the minimum. This combined method can help us to find the minimum even if the initial point is far away from this minimum. If the initial point is extremely far away from minimum, the result can be diverged to $inf$ because the evaluation of the function with exponential can not be saved in np.float64."
      ]
    }
  ]
}